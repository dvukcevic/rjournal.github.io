---
title: Estimating Heteroskedastic and Instrumental Variable Models for Binary Outcome
  Variables in R
abstract: "The objective of this article is to introduce the package \\CRANpkg{Rchoice}
  which provides functionality for estimating heteroskedastic and instrumental variable
  models for binary outcomes, whith emphasis on the calculation of the average marginal
  effects. To do so, I introduce two new functions of the \\CRANpkg{Rchoice} package
  using widely known applied examples. I also show how users can generate publication-ready
  tables of regression model estimates. \n"
draft: no
author:
- name: Mauricio Sarrias
  affiliation: Universidad de Talca
  address:
  - Facultad de Econom√≠a y Negocios (FEN)
  - Universidad de Talca, Talca, Chile
  url: https://www.msarrias.com
  orcid: 0000-0001-5932-4817
  email: mauricio.sarrias@utalca.cl
type: package
output:
  rjtools::rjournal_web_article:
    self_contained: yes
    toc: no
bibliography: IVHet.bib
preamble: |-
  \usepackage{longtable, booktabs, dcolumn, multirow, caption}
  \usepackage{graphicx}
  \usepackage{float}
  \usepackage{amsmath,amssymb,bm,babel,bbm}
  \newcommand{\calA}{\mathcal{A}}
  \newcommand{\calB}{\mathcal{B}}
  \newcommand{\calC}{\mathcal{C}}
  \newcommand{\calD}{\mathcal{D}}
  \newcommand{\calE}{\mathcal{E}}
  \newcommand{\calF}{\mathcal{F}}
  \newcommand{\calG}{\mathcal{G}}
  \newcommand{\calH}{\mathcal{H}}
  \newcommand{\calI}{\mathcal{I}}
  \newcommand{\calJ}{\mathcal{J}}
  \newcommand{\calK}{\mathcal{K}}
  \newcommand{\calL}{\mathcal{L}}
  \newcommand{\calM}{\mathcal{M}}
  \newcommand{\calN}{\mathcal{N}}
  \newcommand{\calO}{\mathcal{O}}
  \newcommand{\calP}{\mathcal{P}}
  \newcommand{\calQ}{\mathcal{Q}}
  \newcommand{\calR}{\mathcal{R}}
  \newcommand{\calS}{\mathcal{S}}
  \newcommand{\calT}{\mathcal{T}}
  \newcommand{\calU}{\mathcal{U}}
  \newcommand{\calV}{\mathcal{V}}
  \newcommand{\calW}{\mathcal{W}}
  \newcommand{\calX}{\mathcal{X}}
  \newcommand{\calY}{\mathcal{Y}}
  \newcommand{\calZ}{\mathcal{Z}}
  \newcommand{\mA}{\mathbf A}
  \newcommand{\va}{\mathbf a}
  \newcommand{\mB}{\mathbf B}
  \newcommand{\vb}{\mathbf b}
  \newcommand{\mC}{\mathbf C}
  \newcommand{\vc}{\mathbf c}
  \newcommand{\mD}{\mathbf D}
  \newcommand{\vd}{\mathbf d}
  \newcommand{\mE}{\mathbf E}
  \newcommand{\ve}{\mathbf e}
  \newcommand{\mF}{\mathbf F}
  \newcommand{\vf}{\mathbf f}
  \newcommand{\mG}{\mathbf G}
  \newcommand{\vg}{\mathbf g}
  \newcommand{\mH}{\mathbf H}
  \newcommand{\vh}{\mathbf h}
  \newcommand{\mI}{\mathbf I}
  \newcommand{\vi}{\mathbf i}
  \newcommand{\mJ}{\mathbf J}
  \newcommand{\vj}{\mathbf j}
  \newcommand{\mK}{\mathbf K}
  \newcommand{\vk}{\mathbf k}
  \newcommand{\mL}{\mathbf L}
  \newcommand{\vl}{\mathbf l}
  \newcommand{\mM}{\mathbf M}
  \newcommand{\vm}{\mathbf m}
  \newcommand{\mN}{\mathbf N}
  \newcommand{\vn}{\mathbf n}
  \newcommand{\mO}{\mathbf O}
  \newcommand{\vo}{\mathbf o}
  \newcommand{\mP}{\mathbf P}
  \newcommand{\vp}{\mathbf p}
  \newcommand{\mQ}{\mathbf Q}
  \newcommand{\vq}{\mathbf q}
  \newcommand{\mR}{\mathbf R}
  \newcommand{\vr}{\mathbf r}
  \newcommand{\mS}{\mathbf S}
  \newcommand{\vs}{\mathbf s}
  \newcommand{\mT}{\mathbf T}
  \newcommand{\vt}{\mathbf t}
  \newcommand{\mU}{\mathbf U}
  \newcommand{\vu}{\mathbf u}
  \newcommand{\mV}{\mathbf V}
  \newcommand{\vv}{\mathbf v}
  \newcommand{\mW}{\mathbf W}
  \newcommand{\vw}{\mathbf w}
  \newcommand{\mX}{\mathbf X}
  \newcommand{\vx}{\mathbf x}
  \newcommand{\mY}{\mathbf Y}
  \newcommand{\vy}{\mathbf y}
  \newcommand{\mZ}{\mathbf Z}
  \newcommand{\vz}{\mathbf z}
  \newcommand{\valpha}{\boldsymbol \alpha}
  \newcommand{\vbeta}{\boldsymbol \beta}
  \newcommand{\vgamma}{\boldsymbol \gamma}
  \newcommand{\vdelta}{\boldsymbol \delta}
  \newcommand{\vepsi}{\boldsymbol \epsi}
  \newcommand{\vvarepsilon}{\boldsymbol \varepsilon}
  \newcommand{\vzeta}{\boldsymbol \zeta}
  \newcommand{\veta}{\boldsymbol \eta}
  \newcommand{\vtheta}{\boldsymbol \theta}
  \newcommand{\viota}{\boldsymbol \iota}
  \newcommand{\vkappa}{\boldsymbol \kappa}
  \newcommand{\vlambda}{\boldsymbol \lambda}
  \newcommand{\vmu}{\boldsymbol \mu}
  \newcommand{\vnu}{\boldsymbol \nu}
  \newcommand{\vxi}{\boldsymbol \xi}
  \newcommand{\vpi}{\boldsymbol \pi}
  \newcommand{\vrho}{\boldsymbol \rho}
  \newcommand{\vsigma}{\boldsymbol \sigma}
  \newcommand{\vtau}{\boldsymbol \tau}
  \newcommand{\vupsilon}{\boldsymbol \upsilon}
  \newcommand{\vphi}{\boldsymbol\phi}
  \newcommand{\vchi}{\boldsymbol \chi}
  \newcommand{\vpsi}{\boldsymbol \psi}
  \newcommand{\vomega}{\boldsymbol \omega}
  \newcommand{\mGamma}{\boldsymbol \varGamma}
  \newcommand{\mDelta}{\boldsymbol \varDelta}
  \newcommand{\mTheta}{\boldsymbol \varTheta}
  \newcommand{\mLambda}{\boldsymbol \varLambda}
  \newcommand{\mXi}{\boldsymbol \varXi}
  \newcommand{\mPi}{\boldsymbol \varPi}
  \newcommand{\mSigma}{\boldsymbol \varSigma}
  \newcommand{\mUpsilon}{\boldsymbol \varUpsilon}
  \newcommand{\mPhi}{\boldsymbol \varPhi}
  \newcommand{\mPsi}{\boldsymbol \varPsi}
  \newcommand{\mOmega}{\boldsymbol \varOmega}
  \newcommand{\vzeros}{\boldsymbol{0}}
date: '2023-11-08'
date_received: '2022-11-03'
volume: 15
issue: 2
slug: RJ-2023-050
journal:
  lastpage: 293
  firstpage: 263

---


# Introduction

Often, applied researchers in different fields deal with binary (probit and logit) models that exhibit heteroskedasticity (the error variance is not homogeneous across  individuals), or with endogenous variables.^[In econometrics, endogeneity refers to situations in which an explanatory variable is correlated with the error term. The common sources of endogeneity are omitted variables, simultaneity, and measurement error.] In both cases, the standard binary logit and probit estimator will be inconsistent, which can lead to misleading conclusions [@yatchew1985specification; @wooldridge2010econometric].^[Inconsistency means that the estimator will not converge in probability to the true parameter.] 

One widely used estimator to address heteroskedastic disturbances in the realm of binary outcomes is the fully parametric multiplicative heteroskedastic binary model [@keele2006difficult]. This model assumes that the error term's variance depends on specific known covariates. For example, @alvarez1995american use a heteroskedastic probit model to show that policy choices about abortion are heterogeneous due to unequal variances.^[For other applications see @knapp1992analysis and @williams2009using.] 

If some of the regressor is endogenous, approaches such as the control function [CF, @wooldridge2015control] or the maximum likelihood estimator [MLE, @newey1987efficient; @rivers1988limited] allow to remediate the inconsistent estimates using an instrumental variables (IV) approach.

Routines for heteroskedastic and IV models exist in commercial software such as Stata [@Stata] and LIMDEP [@LIMDEP]. One advantage of Stata is that its command \code{margins} allows such models to quickly and flexibly compute marginal effects. This is very attractive for users who need to produce and export tables of estimates in Latex or other formats. 

In this article, I review the main approaches and functions in R to estimate heteroskedastic and IV models for binary outcomes, with a special focus on applied examples and the computation of the marginal effects.  Additionally, this article introduces two new functions of the \CRANpkg{Rchoice} package [@sarrias2016discrete] that allow estimating both types of models. The first function, `hetprob()`, estimates binary dependent variable models assuming a parametric form for the heteroskedasticity. The model can be either the probit or logit model and the parameters are estimated by Maximum Likelihood (ML), which find the parameter values that make the observed data most probable under the assumptions of the statistical model. 

The second function, `ivpml()`, estimates binary probit models with endogenous continuous variables using also the ML approach. As an additional feature, \CRANpkg{Rchoice} also provides functions to compute the average marginal effects for both models under different modelling approaches: categorical variables, interactions terms, and quadratic variables. The package can also be used in concert with the \CRANpkg{memisc} package [@elff2012memisc], which produces publication-ready tables of regression model estimates. Finally, I show that both functions produce the same estimates as the corresponding Stata commands.^[Stata codes for replicating the main results of this article are presented in **Appendix C** and **Appendix D**. Do files are available in the supplemental material.] 

The function `hetprob()` is intended to complement other related packages in R. For example, the packages \CRANpkg{glmx} [@glmx] and \CRANpkg{oglmx} [@oglmx] also allow to estimate heteroskedastic binary models using MLE. The latter has the advantage of being able to compute the marginal effects. However, the current version does not allow to identify functions of variables that enter the equations for the mean and standard equations, interaction terms, or polynomials. The `ivpml()` function provides the MLE for the probit model and hence complements the R package \CRANpkg{ivprobit} [@zaghdoudi2018ivprobit] which provides a two-step procedure. Another is the \CRANpkg{LARF} package [@LARF], which estimates local averages response functions for binary treatments and binary instruments. 

# Models

## Heterokedastic binary model

The multiplicative heterokedastic binary model (also known as the location-scale binary model) for cross-sectional data has the following structure [@williams2009using]:^[Multiplicative exponential heteroskedasticity was first proposed by @harvey1976estimating for linear models. For identification of the multiplicative heterokedastic binary model see @carlson2019parametric.]
\begin{align}
	y_i^* & = \vx_i^\top \vbeta + \epsilon_i (\#eq:sthet1), \\
	\textrm{Var}(\epsilon_i| \vz_i) & = \sigma_i^2 = \sigma_{\epsilon}^2\left[\exp\left(\vz_i^\top\vdelta\right)\right]^2, 
	(\#eq:sthet2)  
\end{align}
where $y_i^*$ is the latent (unobserved) response variable for individual $i = 1,...,n$, $\vx_i$ is a $k$-dimensional vector of explanatory variables determining the latent variable $y_i^*$, $\vbeta$ is the vector of parameters, and $\epsilon_i$ is the error term distributed either normally or logistically with $\mathbf{E}(\epsilon_i|\vz_i, \vx_i) = 0$ and multiplicative heterokedastic variance $\textrm{Var}(\epsilon_i|\vz_i)  = \sigma_i^2 , \forall i = 1,...,n$ [@harvey1976estimating]. The variance for each individual is modeled parametrically assuming that it depends on a $p$-dimensional vector of observed variables $\vz_i$, whereas $\vdelta$ is the vector of coefficients associated with each variable. It is important to emphasize that $\vz_i$ does not include a constant, otherwise the parameters are not identified [@greene2010modeling]. 

Since we do not observe $y_{i}^*$, we need a rule that relates the binary variable that we actually observe, $y_i$,  to the latent variable. As it is standard, we use the following rule:
\begin{equation}
	y_i = \begin{cases}
		1 & \mbox{if $y_i^*> 0$}, \\
		0 & \mbox{otherwise}.
	\end{cases}
	(\#eq:assign) 
\end{equation}

Using Equations \@ref(eq:sthet1), \@ref(eq:sthet2) and \@ref(eq:assign), the probability of observing $y_i = 1$ is:
\begin{equation}
	\Pr(y_i = 1|\vx_i, \vz_i) = F\left(\frac{\vx_i^\top\vbeta}{\exp(\vz_i^\top\vdelta)}\right), 
	(\#eq:probhet)
\end{equation}	
where $F(\cdot)$ is either $\Phi(\cdot)$, that is, the cumulative distribution function (CDF) for the standard normal distribution, such that $\sigma_{\epsilon}^2 = 1$, or $\Lambda(\cdot) = \frac{\exp(\cdot)}{1 + \exp(\cdot)}$, where $\Lambda(\cdot)$ represents the CDF for the standard logistic distribution, so that $\sigma_{\epsilon}^2 = \pi^2/3$.

Let $\vtheta$ be the $(k + p)$-dimensional vector of all parameters. The vector $\vtheta$ can be estimated using the Maximum Likelihood procedure. Using Equation \@ref(eq:probhet), the MLE is the value of the parameters that maximizes the following log-likelihood function:^[The analytic gradient and Hessian for the multiplicative heterokedastic binary model used by \CRANpkg{Rchoice} are presented in **Appendix A**.]
\begin{equation*}
	\widehat{\vtheta}_{ML} \equiv \underset{\vtheta \in \mTheta}{\textrm{argmax}}\; \sum_{i = 1}^n \ln \left\lbrace \left[1- F\left(\frac{\vx_i^\top\vbeta}{\exp(\vz_i^\top\vdelta)}\right)\right]^{1-y_i}\left[F\left(\frac{\vx_i^\top\vbeta}{\exp(\vz_i^\top\vdelta)}\right)\right]^{y_i}\right\rbrace.
\end{equation*}

As in any non-linear model, the estimated coefficients alone cannot be interpreted as marginal changes on $\textrm{Pr}(y_i = 1|\vx_i, \vz_i)$. Let $w_k$ be a continuous variable appearing in both $\vx$ and $\vz$, then the partial effect is [see @greene2003econometric]:
\begin{equation}
	\frac{\partial \textrm{Pr}(y_i = 1| \vx_i, \vz_i)}{\partial w_{ik}} = f\left(\frac{\vx_i^\top\vbeta}{\exp\left(\vz_i^\top\vdelta\right)}\right)\left(\frac{\beta_k - (\vx_i^\top\vbeta)\delta_k}{\exp\left(\vz_i^\top\vdelta\right)}\right),
	(\#eq:apehet)
\end{equation}
where $f(\cdot)$ is the probability density function (PDF) for the standard normal or standard logistic distribution. The average partial effect (APE) can be consistently estimated as follows:
\begin{equation}
  \widehat{\textrm{APE}}_k = \frac{1}{n}\sum_{i = 1}^nf\left(\frac{\vx_i^\top\widehat{\vbeta}}{\exp\left(\vz_i^\top\widehat{\vdelta}\right)}\right)\left(\frac{\widehat{\beta}_k - (\vx_i^\top\widehat{\vbeta})\widehat{\delta}_k}{\exp\left(\vz_i^\top\widehat{\vdelta}\right)}\right),
  (\#eq:apehethat)
\end{equation}
and their standard error can be estimated either by delta method or bootstrap. The delta method provides an analytic approximation for the standard errors based on the asymptotic variance-covariance matrix of the MLE. The bootstrap is non-parametric resampling technique, which involves generating a large number of resampled datasets (bootstrap samples) and estimating \@ref(eq:apehethat) for each sample. For further details see @wooldridge2010econometric.

Finally, a likelihood-ratio (LR) or Wald test can be performed to test the null hypothesis of homoskedasticity: $H_0: \vdelta  = \vzeros$.

## Probit models with endogenous continous variable

Consider the following two-equation model:
\begin{eqnarray}
	y_{1i}^*  &= &  \vx_{1i}^\top\vbeta_{1} +  \gamma y_{2i} + \epsilon_{i} = \vx_i^\top\vbeta + \epsilon_i,   (\#eq:firsteq)\\
	y_{2i}   & = & \vx_{1i}^\top \vdelta_1 + \vx_{i2}^\top \vdelta_2 + \upsilon_{i} = \vz_i^\top\vdelta +\upsilon_i, (\#eq:secondeq) \\
  y_{1i}   & = & \mathbf{1}\left[y_{1i}^* > 0\right], (\#eq:binassign)
\end{eqnarray}
where $i = 1, ..., n$,  $y_{1i}^*$ is a latent (unobserved) response variable for individual $i$ and we observe $y_{1i} = 1$ if and only if $\mathbf{1}\left[y_{1i}^* > 0\right]$,  $y_{2i}$ is the **continuous endogenous** variable, $\vx_{i1}$ is a $k_1$-dimensional vector of predetermined (exogenous) variables, $\vx_{i2}$ is a $k_2$-dimensional vector of additional (exogenous) instruments, $\vx_i = \left(\vx_{1i}^\top, y_{2i}\right)^\top$ is a $k\times 1$ column vector such that $k = k_1 + 1$, and $\vz_i = \left(\vx_{1i}^\top, \vx_{2i}^\top\right)^\top$ is a $p\times 1$ vector where $p = k_1 + k_2$. Equation \@ref(eq:firsteq) is the structural equation, whereas Equation \@ref(eq:secondeq) is the first-stage equation. Further, assume that $(\epsilon, \upsilon)$ are distributed as bivariate normal with zero mean. 

### Two-step approach

The simplest approach for estimating the parameters of Equation \@ref(eq:firsteq) and \@ref(eq:secondeq) is using a two-step procedure [@rivers1988limited] also known as Control Function (CF) approach [@wooldridge2015control]. Under joint normality of $(\epsilon, \upsilon)$, we can write $\epsilon$ as a function of $\upsilon$ as follows:^[If $x\sim N(\mu, \sigma^2)$, then we can write $x_i = \mu + \sigma u_i$, where $u_i \sim N(0, 1)$.]
\begin{equation}
  \epsilon_i|\upsilon_i = \frac{\sigma_{\epsilon}}{\sigma_{\upsilon}}\rho\upsilon_i + \eta_i,
  (\#eq:epsilon1)
\end{equation}
where $\textrm{Var}(\epsilon_i) = \sigma_{\epsilon}^2$, $\textrm{Var}(\upsilon_i) = \sigma_{\upsilon}^2$,  $\eta_i \sim N\left[0, (1 - \rho^2)\sigma_{\epsilon}^2\right]$ and  $\rho = \textrm{Cov}(\epsilon_i, \upsilon_i) / (\sigma_{\epsilon}\cdot \sigma_{\upsilon})$. If $\rho = 0$,  $y_{2}$ is exogenous and the traditional probit model will deliver consistent estimates. For identification, we need to set $\textrm{Var}(\epsilon_i) = 1$. Then Equation \@ref(eq:epsilon1) can be re-written as:
\begin{equation}
 \epsilon_i = \lambda\upsilon_i + \eta_i,
 (\#eq:epsilonupsilon)
\end{equation}
where $\eta_i \sim N\left[0, (1 - \rho^2)\right]$ and $\lambda = \textrm{Cov}(\epsilon_i, \upsilon_i) / \sigma_{\upsilon}^2$. Inserting Equation \@ref(eq:epsilonupsilon) in the latent Equation \@ref(eq:firsteq) yields:
\begin{equation*}
	y_{1i}^*  =   \vx_{1i}^\top\vbeta_{1} +  \gamma y_{2i} + \lambda\upsilon_i + \eta_i, 
\end{equation*}
and the probability of observing $y_{1i} = 1$ is:
\begin{equation}
		\textrm{Pr}(y_{1i} =1|  y_{2i}, \vz_i, \upsilon_i)  = \textrm{Pr}(y_{1i}^* > 0 | y_{2i}, \vz_i, \upsilon_i) = \Phi\left(\vx_{1i}^\top\vbeta_{1}^* + \gamma^* y_{2i}+ \lambda^*\upsilon_i\right).
		(\#eq:pi)
\end{equation}

Thus, if we knew $\upsilon_i$, a probit of $y_1$ on $\vx$ and $\upsilon$ would consistently estimate the scaled parameters $\vbeta^*_1 = \vbeta_1/\sqrt{1 - \rho^2}$,  $\gamma^* = \gamma/\sqrt{1 - \rho^2}$, and $\lambda ^* = \lambda/\sqrt{1 - \rho^2}$. Using this idea, the estimation procedure is as follows [see @wooldridge2010econometric, sect. 15.7.2]:

- Run an OLS regression of $y_2$ on $\vz$ (Equation \@ref(eq:secondeq)) and compute the residuals $\widetilde{\upsilon}_i = y_{2i} - \vz_i^\top\widetilde{\vdelta}$. Both $\widetilde{\vdelta}$ and $\widetilde{\upsilon}$ are consistently estimated. 
- Run the probit $y_1$ on $\vx_1$, $y_2$ and $\widetilde{\upsilon}$ to get consistent estimators of the scaled coefficients $\vbeta^*$, $\gamma^*$ and $\lambda ^*$. 

Note that the term control function comes from the fact that the inclusion of $\widetilde{\upsilon}$ in the second step controls for the correlation between $\epsilon_i$ and $\upsilon_i$. 

Some of the structural parameters can be recovered after the two-step procedure. Since $\sigma_{\epsilon} = 1$,  $\rho = \textrm{Cov}(\epsilon_i, \upsilon_i) /  \sigma_{\upsilon} = \lambda \cdot \sigma_{\upsilon}$. Thus, an estimate of $\rho$ can be recovered from:
\begin{equation}
\widehat{\rho} = \widehat{\lambda}^*\cdot\widetilde{\sigma}_{\upsilon},
(\#eq:estimaterho)
\end{equation}
where $\widehat{\lambda}^*$ is the probit estimate of $\lambda^*$ and $\widetilde{\sigma}_{\upsilon}$ is the square root of the usual error variance estimator from the first-stage regression. The unscaled parameters can also be recovered using the two-stage estimates. For instance, since $\gamma^* = \gamma / \sqrt{(1 - \rho^2)}$, and using our result in Equation \@ref(eq:estimaterho), then $\widehat{\gamma} = \widehat{\gamma}^*\left[1 - \left(\widehat{\lambda}^*\cdot\widetilde{\sigma}_{\upsilon}\right)^2\right]^{1/2}$.

As explained by @wooldridge2010econometric, the usual probit $z$-statistic on $\widetilde{\upsilon}$ is a valid test of the null hypothesis that $y_2$ is exogenous: $H_0:\lambda^* = 0$.^[Under the null $H_0:\lambda^* = 0$ it is true that $\epsilon = \upsilon$ and therefore the distribution of $\upsilon$ does not play any role under the null.] However, the estimated variance-covariance matrix of the probit model does not deliver correct standard errors for the rest of the parameters since it does not include the sampling variability of $\widehat{\vdelta}$ when $\lambda \neq 0$.

Following @wooldridge2015control, the APEs are obtained by taking either derivatives or differences (depending on whether the explanatory variable is continuous or discrete) of the Average Structural Function (ASF) given by:
\begin{equation}
  \textrm{ASF}(\vx_1, y_2) = \mathbf{E}_{\upsilon}\left[\Phi\left(\vx_{1i}^\top\vbeta_{1}^* + \gamma^* y_{2i}+ \lambda^*\upsilon_i\right)\right].
  (\#eq:ASFivprobit)
\end{equation}

This function averages out the first-stage residuals $\upsilon_i$, purging the model of endogeneity. Under the weak law of large numbers, a consistent estimator for $\textrm{ASF}(\vx_1, y_2)$ in Equation \@ref(eq:ASFivprobit) is:
\begin{equation}
\widehat{\textrm{ASF}} = \frac{1}{n}\sum_{i=1}^n\Phi\left(\vx_{1i}^\top\widehat{\vbeta}_{1}^* + \widehat{\gamma}^* y_{2i}+ \widehat{\lambda}^*\upsilon_i\right),
\end{equation}
which incorporates the estimated unobservables from the first stage without perturbing them. Hence, to estimate the APE for $y_2$ we can compute:
\begin{equation}
\widehat{\textrm{APE}}_{y_2} = \widehat{\gamma}^*\frac{1}{n}\sum_{i=1}^n\phi\left(\vx_{1i}^\top\widehat{\vbeta}_{1}^* + \widehat{\gamma}^* y_{2i}+ \widehat{\lambda}^*\upsilon_i\right).
(\#eq:apey2cf)
\end{equation}
where $\phi(\cdot)$ is the standard normal density function. A standard error for this $\widehat{\textrm{APE}}$ can be obtained via the delta method or bootstrap. 

## Maximum Likelihood approach 

We can also estimate the parameters using the MLE. To derive the log-likelihood function, we need to find the joint distribution $f(y_{1i}, y_{2i}|\vz) = f(y_{1i}|y_{2i}, \vz_i)f(y_{2i}|\vz_i)$. Under the joint normality, $y_{2i}|\vz_i \sim N(\vz_i^\top\vdelta, \sigma_{\upsilon}^2)$ and its conditional marginal density is [@wooldridge2014quasi]:
\begin{equation}
f(y_{2i}|\vz_i) = \frac{1}{\sigma_{\upsilon}}\phi\left(\frac{y_{2i} -\vz_i^\top\vdelta }{\sqrt{1 - \rho^2}}\right).
(\#eq:marginal)
\end{equation}

Using the fact that the normal distribution is symmetric, the conditional density of $y_{2i}$ given $(y_{2i}, \vz_i)$ can be written as:
\begin{equation}
f(y_{1i}|y_{2i}, \vz_i)= \Phi\left[q_i\cdot\left(\frac{\vx_i^\top\vbeta + \frac{\rho}{\sigma_{\upsilon}}\left(y_{2i} - \vz_i^\top\vdelta\right)}{\sqrt{1 - \rho^2}}\right)\right],
(\#eq:conditional)
\end{equation}
where $q_i = 2y_{2i} - 1$ [see @greene2003econometric]. Using Equations \@ref(eq:marginal) and \@ref(eq:conditional), the joint probability for each individual $i$ is:
\begin{equation}
f(y_{1i}, y_{2i}|\vz_i;\vtheta) = \Phi\left[q_i\cdot\left(\frac{\vx_i^\top\vbeta + \frac{\rho}{\sigma_{\upsilon}}\left(y_{2i} - \vz_i^\top\vdelta\right)}{\sqrt{1 - \rho^2}}\right)\right]\frac{1}{\sigma_{\upsilon}}\phi\left(\frac{y_{2i} -\vz_i^\top\vdelta }{\sqrt{1 - \rho^2}}\right).
(\#eq:piml)
\end{equation}

The MLE is a value of the parameter vector that maximizes the following expression:^[The analytic gradient and Hessian for the MLE used by \CRANpkg{Rchoice} are presented in **Appendix B**.]
\begin{equation*}
	\widehat{\vtheta}_{ML} \equiv \underset{\vtheta \in \mTheta}{\textrm{argmax}}\; \sum_{i = 1}^n \ln \left\lbrace \Phi\left[q_i\cdot\left(\frac{\vx_i^\top\vbeta + \frac{\rho}{\sigma_{\upsilon}}\left(y_{2i} - \vz_i^\top\vdelta\right)}{\sqrt{1 - \rho^2}}\right)\right]\frac{1}{\sigma_{\upsilon}}\phi\left(\frac{y_{2i} -\vz_i^\top\vdelta }{\sqrt{1 - \rho^2}}\right)\right\rbrace.
\end{equation*}

After the parameters are estimated, the APE for the endogenous variable can be estimated as:
\begin{equation}
\widehat{\textrm{APE}}_{y_2} = \frac{\widehat{\vgamma}}{\sqrt{1 - \widehat{\rho}^2}}\frac{1}{n}\sum_{i=1}^n\phi\left(\frac{\vx_i^\top\widehat{\vbeta} + \frac{\widehat{\rho}}{\widehat{\sigma}_{\upsilon}}\widehat{\upsilon}_i}{\sqrt{1 - \widehat{\rho}^2}}\right).
(\#eq:ME1y2ivprobit)
\end{equation}

A second option would be to compute the effect for the structural model assuming that endogeneity does not exist (the values of the covariates are given and fixed). In this case, the APE for the endogenous variable is computed as:
\begin{equation}
\widehat{\textrm{APE}}_{y_2} = \widehat{\vgamma}\frac{1}{n}\sum_{i=1}^n\phi\left(\vx_i^\top\widehat{\vbeta}\right).
(\#eq:ME2y2ivprobit)
\end{equation}

# Applications

## Heteroskedastic binary models

### Promotion of scientists

To show how R can be used to fit heteroskedastic binary response models, I first use @allison1999comparing's dataset called ```tenure.cvs`'' [see also @williams2010fitting]. The data consists of observations of the careers of university professors over time, tracking multiple cross-sectional and longitudinal indicators including gender, the number of published article, and quality of department, among others. 

We can load the dataset into R as follows:

```{r tenure-data, message = FALSE}
tenure_data <- read.csv(file = 'tenure.csv')
```

Following @allison1999comparing and @williams2009using I focus on whether women get a lower payoff from their published work than men. First, I estimate a binary logit model using the `glm()` function for men and women separately, where the structural model is given by
\begin{equation*}
\begin{aligned}
  \text{tenure}^* &= \beta_0 + \beta_1\text{year} + \beta_2 \text{year}^2 + \beta_3\text{select}+\beta_4\text{articles}+\beta_5 \text{prestige} + \epsilon, \\
  \text{tenure} & = \mathbf{1}\left[\text{tenure}^* > 0\right],   
\end{aligned}
\end{equation*}
where $\epsilon$ is distributed logistically with mean 0 and variance $\pi^2/3$. The dependent variable, `tenure`, is whether an assistant professor was promoted in that year, and 0 otherwise, `year` is the number of years since the beginning of the assistant professorship, `select` is a measure of undergraduate selectivity of the colleges where scientists received their bachelor's degree, `articles` is the cumulative number of articles published by the end of each person-year, and `prestige` is a measure of prestige of the department in which scientist was employed. To obtain similar results as @allison1999comparing, I restrict the sample to `year <= 10`. Thus, each person has one record per year of service as an assistant professor, for as many as ten years.

```{r logit}
sub_data <- subset(tenure_data, year <= 10)
logit_m <- glm(tenure ~  year + I(year^2) + select + articles + prestige, 
               subset = (female == 0),
               data   = sub_data,
               family = binomial(link = "logit"))
logit_w <- glm(tenure ~  year + I(year^2) + select + articles + prestige, 
               subset = (female == 1), 
               data   = sub_data, 
               family = binomial(link = "logit"))
```

To present the results I use the `mtable()` function from \CRANpkg{memisc} package [@elff2012memisc]. 

```{r table-logit, message = FALSE, warning=FALSE}
library("memisc")
mtable("Logit for men" = logit_m,
       "Logit for women" = logit_w, 
       summary.stats = c("Log-likelihood", "AIC", "BIC", "N"))
```

From previous output, it can be noticed that the coefficient of `articles` for men is approximately twice as large as for women: `0.074` vs `0.034`. One possible conclusion we could draw from this result is that women suffer from discrimination. That is, the return per additional article on the propensity to get a promotion is on average lower for women, holding other things constant. However, @allison1999comparing notes that this result might be due to variance error term differences. For example, women might have more heterogeneous career patterns than men due to unobserved factors affecting promotion. In particular, assume that we have the following model for men ($M$) and women ($W$):
\begin{align*}
  y_{iM}^* & = \vx_{iM}^\top\vbeta + \epsilon_{iM}, \\
  y_{iW}^* & = \vx_{iW}^\top\vbeta + \epsilon_{iW}, \\
  \epsilon_{iM} & \sim \Lambda(0, \sigma_{M}^2), \\
  \epsilon_{iW} & \sim \Lambda(0, \sigma_{W}^2),
\end{align*}
where $\Lambda(\cdot)$ is the logistic CDF. Both men and women have the same coefficients, $\vbeta$, in the propensity to be promoted, but different scales,  $\sigma_{M}^2\neq \sigma_{W}^2$. Note that the logit model identifies $\beta = \frac{\alpha}{\sigma}$.  Thus, if women have greater variance than men, $\sigma_W > \sigma_M$, their coefficient will be smaller, assuming similar return to productivity. To allow for such possibility, @williams2009using suggests fitting a heteroskedastic logit (HET-Logit) model where the standard deviation of the error term is modeled as
\begin{equation*}
\sigma_i = \exp(\delta \cdot \texttt{female}_i).
\end{equation*}

This model can be estimated in R using the `hetglm()` function from \CRANpkg{glmx} package or `hetprob()` function from \CRANpkg{Rchoice} package. The syntax to fit the model using `hetprob()` is the following

```{r het.logit, message = FALSE}
library("Rchoice")
het_logit <- hetprob(tenure ~ factor(female) + year + I(year^2) + select +  
                       articles + prestige | factor(female), 
                     data = sub_data, 
                     link = "logit")
```

Similarly to `hetglm()` function, the `formula` argument of `hetprob()` has the form `y ~ x | z`, where `y` is the binary response variable, `x` are the explanatory covariates, and `z` are the covariates affecting the variance of the error term. The argument `link` indicates whether a logit (`link = "logit"`) or probit (`link = "probit"`) model should be fitted. 

The output is the following: 

```{r sum.het.logit}
summary(het_logit)
```

The results using `hetglm()` are the following

```{r hetglm}
library("glmx")
het_glmx <- hetglm(tenure ~ factor(female) + year + I(year^2) + select +  
                       articles + prestige | factor(female), 
                     data = sub_data, 
                    family = binomial(link = "logit"))
summary(het_glmx)
```

Although the coefficients estimated by both functions are very similar, their standard errors are somewhat different. One potential explanation for this difference is the optimization algorithm used by each function.  `hetprob()` uses Newton-Raphson algorithm available in `maxLik()` function from \CRANpkg{maxLik} package [@henningsen2011maxlik], whereas `hetglm()` uses `nlminb` algorithm as default.

Now, I compare the logit and Het-Logit estimates using `mtable()` function.^[`mtable()` does not support objects of class `hetglm`.] The following output presents the estimates. 

```{r table-het1}
mtable("Logit for men"   = logit_m,
       "Logit for women" = logit_w, 
       "Heteroskedastic" = het_logit,
       summary.stats = c("Log-likelihood", "AIC", "BIC", "N"))
```

The estimated coefficients for the HET-Logit model indicate that being a woman increases the variance of the error term ($\widehat{\delta}$ = `0.302`) and decreases the propensity to be promoted ($\widehat{\beta}_6$ = `-0.939`).

Using the estimate $\widehat{\delta}$, we can also compute how much the disturbance standard deviation differ by gender. Note that the standard deviation of the error term for women is $\sigma_W = \exp(0.302)$, whereas for men is $\sigma_M = \exp(0) = 1$. Then,

```{r allinson-delta}
sigma_w <- exp(coef(het_logit)["het.factor(female)1"])
(1 -  sigma_w) / sigma_w
```

This result implies that the standard deviation of the disturbance for men is 26\% lower than the standard deviation for women. Conversely, this also means that the standard deviation of the residuals is $\exp(0.302) = 1.35$ times larger for women compared to men [@williams2009using; @williams2010fitting]. The 95\%-CI for this ratio can be computed using the delta method by `deltaMethod()` function from \CRANpkg{car} package [@fox2013hypothesis]:

```{r allinson-delta-ci, message = FALSE}
library("car")
sharef <- "(1 - exp(`het.factor(female)1`)) / exp(`het.factor(female)1`)"
deltaMethod(het_logit, sharef)
```

So far, the HET-Logit estimates suggest that there are gender differences in both the dependent variable and in the variance of the error term. However, the estimated coefficients do not allow us to conclude whether women have a lower return than men for productivity. To give some insights about this question, I estimate a HET-Logit model including the interaction between `female` and `articles` in the choice equation: 

```{r interaction}
het_logit2 <- hetprob(tenure ~ factor(female) + year + I(year^2) + select +  
                        articles + prestige + factor(female)*articles | 
                        factor(female), 
                      data = sub_data, 
                      link = "logit")
print(summary(het_logit2), digits = 3)
```
The coefficient for `female * articles` is not statistically significant when residual variation by gender is involved. As argued by @allison1999comparing, this result proposes dissimilarities in productivity returns between males and females resulting from variability in unobserved factors rather than discriminatory influences.

Once we have fitted a model, we can use the `predict()` command to obtain the predicted probability and the predicted scale factor, $\widehat{\sigma}_i$, which can be readily used for visualization as shown in Figure \@ref(fig:plothet). The following lines plots the distribution of both measures: 

```{r plothet, fig.cap = "Distribution of predicted probability and predicted sigma", fig.width=6, fig.height=3.5}
par(mfrow = c(1, 2))
hist(predict(het_logit2, type = "pr"), 
     main = "Predicted probabilities", 
     xlab = "Probabilities")
hist(predict(het_logit2, type = "sigma"), 
     main = "Predicted sigma", 
     xlab = "Sigma")
```

An additional feature of \CRANpkg{Rchoice} package is that it allows to estimate the APEs for heteroskedastic binary models, as in Equation \@ref(eq:apehethat), using `effect()` function. Similarly to command `margins()` from \CRANpkg{margins} package [@margins] or `avg_slopes()` from \CRANpkg{marginaleffects} package [@marginaleffects], this function takes into account whether the variables are continuous, categorical or both. The user must specify categorical variables using `factor()` in the `formula` argument; otherwise, the `effect()` function will assume that the variable is continuous, when the variable may already be a `factor` in the dataset. In the following lines, we compute the APEs for a HET-Probit and HET-Logit model.^[The Jacobian matrix is computed numerically using `jacobian()` function from \CRANpkg{numDeriv} package [@numderiv].] The results are the following:

```{r het.me}
eff_logit  <- effect(het_logit2)
het_probit <- hetprob(tenure ~ factor(female) + year + I(year^2) + select +  
                        articles + prestige + factor(female)*articles | 
                        factor(female),              
                      data = sub_data,                        
                      link = "probit")
eff_probit <- effect(het_probit)
mtable(eff_probit,
       eff_logit)
```

The APEs are very close to each other and statistically significant. According to the HET-Probit estimates, one additional published article increases the probability of being promoted by 0.6 percent points, whereas being a woman decreases the probability of promoted by 3.1\%. 

### Labor participation

Our second example is a replication of @greene2003econometric's example 17.7 based on the dataset ```mroz.cvs`''. This dataset is based on a cross-section data on the wages of 428 working, married women, originating from the 1976 Panel Study of Income Dynamics (PSID), which can be loaded as follows: 

```{r load-mroz}
mroz <- read.csv(file = 'mroz.csv')
mroz$kids <- with(mroz, factor((kidslt6 + kidsge6) > 0,
                               levels = c(FALSE, TRUE), 
                               labels = c("no", "yes")))
mroz$finc <-  mroz$faminc / 10000
```

Using this data, @greene2003econometric estimates the following HET-Probit model for women labor participation:
\begin{align}
\text{inlf}^* & = \beta_0 + \beta_1\text{age}+\beta_2\text{age}^2 + \beta_3\text{finc} + \beta_4\text{educ} + \beta_5\text{kids} + \epsilon, (\#eq:lfphet1) \\
\epsilon & \sim N(0, \sigma_i^2), \\
\sigma_i & = \exp(\delta_1 \text{kids} + \delta_2 \text{finc}),
(\#eq:lfphet2)
\end{align}
where `inlf` is a dummy variable indicating whether the woman participates in labor force, `age` is age in year, `finc` is family income in 1975 dollars divided by 10,000, `educ` is education in year and `kids` indicates whether children under 18 are present in the household. It is further assumed that `kids` and `finc` affect the variability of the error term. 

The probit, Het-Probit and average marginal effects are estimated as follows:^[@greene2003econometric computes the marginal effects at the mean instead of the average marginal effects.]

```{r greene-17}
labor_hom <- glm(inlf ~  age + I(age^2) + finc + educ + factor(kids), 
                 data = mroz, 
                 family = binomial(link = "probit"))
labor_het <- hetprob(inlf ~  age + I(age^2) + finc + educ + factor(kids) | 
                       factor(kids) + finc,              
                     data = mroz,                        
                     link = "probit")
eff_labor_het <- effect(labor_het)
mtable(labor_hom,
       labor_het,
       eff_labor_het)
```

The results show that family income does not play any role in the choice equation. However, it increases the variability of the error term. APE indicates that an increase of \$10,000 of family income increases the probability of labor force involvement by 6.9\%. There is not enough statistical evidence that proves having children under 18 in the household produces heteroskedasticity. 

We can also use the Wald test provided by `linearHypothesis()` function from \CRANpkg{car} package to test the null hypothesis of homoskedasticity:

```{r test-homokedasticity}
coefs <- names(coef(labor_het))
linearHypothesis(labor_het, coefs[grep("het", coefs)])
```

The null hypothesis of homoskedasticity is rejected at the 5\% with a $\chi_2^2 = 6.533$. 

Supplementary materials provide Stata code (version 16.1) to replicate all the results in this Section. The log file is presented in **Appendix C**. Overall, the results using Stata are exactly the same to those reported by `hetprob()` function from \CRANpkg{Rchoice} package. 

## Instrumental variable probit model

### Control function approach

In this example, and similar to @wooldridge2010econometric, we use the `mroz` sample and assume the following slightly modified model for married women's labor force participation from previous Section:
\begin{align*}
  \text{inlf}^* = &  \beta_0 + \beta_1\text{educ} + \beta_2 \text{exper} + \beta_3\text{exper}^2+\beta_4\text{age}+\beta_5 \text{kidslt6} +\\
                    &  \beta_6 \text{kidsge6}+\beta_7\text{nwifeinc}+ \epsilon, \\
  \text{nwifeinc} = & \delta_0 + \delta_1\text{educ} + \delta_2 \text{exper} + \delta_3\text{exper}^2+\delta_4\text{age}+\delta_5 \text{kidslt6} +\\
                    &  \delta_6 \text{kidsge6}+\delta_7\text{huseduc}+ \upsilon, \\          \text{lfp}   & =  \mathbf{1}\left[\text{lfp}^* > 0\right]
\end{align*}
where `nwifeinc` is the other sources of income (divided by 1,000) and assumed to be endogenous. A just identified IV model is estimated by using husband's education, (`huseduc`), as an instrument for `nwifeinc`. The strong identification assumption here is that husband's schooling is unrelated to factors that affect a married woman's labor force decision once `nwifeinc` and the other variables are accounted for [@wooldridge2010econometric].

When interpreting the results from an IV model, it is important to compare its magnitude with a model that assumes exogeneity. In this example, our benchmark APE for `nwifeinc` is obtained by the standard probit model:

```{r probit-nwincome}
probit <- glm(inlf ~  educ + exper + I(exper^2) + age + kidslt6 + kidsge6 + nwifeinc, 
              data = mroz, 
              family = binomial(link = "probit"))
ape.probit <- mean(dnorm(predict(probit, type = "link"))) * coef(probit)["nwifeinc"]
ape.probit
```

Accordingly, an increase of \$1,000 in other sources of income reduces the labor force participation probability by 0.4\%, holding all other factors  constant.  Note that the same APE, along with its standard error, can also be obtained using `avg_slopes()` command: 

```{r margins}
library("marginaleffects")
avg_slopes(probit, variables = "nwifeinc")
```

I proceed to estimate the model using the CF approach. First, I estimate the first-step equation, which is a linear model, and obtain the residuals $\widetilde{\upsilon}$:

```{r iv-first-step}
fstep   <- lm(nwifeinc ~ educ + exper + I(exper^2) + age + kidslt6 + kidsge6  + huseduc, 
            data = mroz)
mroz$res.hat <- fstep$residuals
```

We can also test the power of the instrument using `linearHypothesis()` function:

```{r F-test}
linearHypothesis(fstep, "huseduc")
```

The first-stage $F$ statistic on `huseduc` is substantially above the traditional cut-off of ten suggesting that the instrument is not weak.

The second-step is computed using `glm()` function and adding the residuals (`res.hat`) as an additional explanatory variable: 

```{r iv-second-step}
sstep   <- glm(inlf ~  educ + exper + I(exper^2) + age + kidslt6 + kidsge6 + nwifeinc + res.hat, 
            data   = mroz, 
            family = binomial(link = "probit")) 
summary(sstep)
```

Since the $z$-statistic for `res.hat` is 1.4, we cannot reject the null hypothesis that `nwifeinc` is exogenous: $H_0:\lambda = 0$. 

An estimate of $\rho$ can be obtained using Equation \@ref(eq:estimaterho) and the following syntax:

```{r compute-rho}
lambda.hat    <- coef(sstep)["res.hat"]
k             <- length(fstep$coefficients)
SSE           <- sum(fstep$residuals^2)
n             <- length(fstep$residuals)
sigma.upsilon <- sqrt(SSE/(n - k))
rho.hat       <- lambda.hat * sigma.upsilon
rho.hat
```

Thus, the estimated correlation using the CF approach is $\widehat{\rho} = 0.279$. It is important to recall that the estimated coefficients for the `sstep` model represent the coefficients scaled by a factor of $1 / \sqrt{1 - \rho^2}$. Moreover, the standard errors from the `sstep` model are biased since they do not consider the sampling error of the first stage. However, we can use `ivprobit()` function from \CRANpkg{ivprobit} package  [@zaghdoudi2018ivprobit] to get the correct standard errors:^[`ivprobit()` uses a minimum chi-squared estimator [@newey1987efficient].]

```{r ivprobit-function}
library("ivprobit")
twostep.probit <- ivprobit(inlf ~  educ + exper + I(exper^2) + age + kidslt6 + kidsge6 | 
                             nwifeinc | educ + exper + I(exper^2) + age + kidslt6 + 
                             kidsge6 + huseduc, 
                           data = mroz)
summary(twostep.probit)
```

The estimates of the `sstep` and `twostep.probit` models are the same, while their standard errors are slightly different. 

The APE for `nwifeinc`---and any other continuous variable---can be computed using Equation \@ref(eq:apey2cf) and its standard error via bootstrap method. Below, I use package \CRANpkg{boot} [@canty2002resampling] to perform the simulation. First, a function called `ape()` is created which returns the APE. The first argument of this function is the dataset, whereas the second argument can be an index vector of the observations in the dataset.

```{r ape}
ape <- function(data, indices){
  d <- data[indices, ]
  # Compute the first-stage regression
  fstep    <- lm(nwifeinc ~ educ + exper + I(exper^2) + age + kidslt6 + kidsge6 + 
                   huseduc, 
                 data = d)
  # Obtain the residuals 
  d$res.hat <- fstep$residuals
  # Compute the second-stage regression
  sstep   <- glm(inlf ~  educ + exper + I(exper^2) + age + kidslt6 + kidsge6 + 
                   nwifeinc + res.hat, 
                 data   = d, 
                 family = binomial(link = "probit"))
  # Compute APE for nwincome
  out <- mean(dnorm(predict(sstep, type = "link"))) * coef(sstep)["nwifeinc"]
  return(out)
}
```

Once we have defined the function `ape()`, we can use the `boot()` function to perform the bootstrap procedure. In the following syntax, `R = 500` resamplings are used and the 90\%-CI interval is obtained using `boot.ci()` function. 

```{r boot, message = FALSE}
library("boot")
set.seed(666)
results <- boot(data = mroz, statistic = ape, R = 500)
results
boot.ci(results, type = "norm", conf = 0.90)
```

The results show that another \$1,000 in other sources of income reduces the labor force participation probability by 1.1 percent points with 90\%-CI $\left[-2, -.09\right]$. This estimate, which is marginally statistically significant,  is about three times larger than the probit estimate that treats `nwifeinc` as exogenous: `-0.04`.

Finally, we can recover the unscaled parameters by multiplying the coefficients by $\sqrt{\left(1 - \widehat{\rho}^2\right)}$ as follows: 

```{r unscaledpar}
coef(sstep) * sqrt(1 - rho.hat^2)
```

### Maximum likelihood estimator

In this Section I estimate the model from previous Section using the MLE. To do so, I use the `ivpml()` function from \CRANpkg{Rchoice} package. The syntax is as follows: 

```{r fiml}
fiml.probit <- ivpml(inlf ~  educ + exper + I(exper^2) + age + kidslt6 + kidsge6 + 
                       nwifeinc | huseduc +  educ + exper + I(exper^2) + age + 
                       kidslt6 + kidsge6, 
                     data = mroz)
```

The syntax of `ivpml()` is similar to that of `ivreg()` function from \CRANpkg{AER} package. The `formula` has two part in the right-hand side, that is, `y ~ x | z` where `y` is the binary response variable, `x` are the regressors ($\vx$ in Equation \@ref(eq:firsteq)), and `z` are the exogenous variables ($\vx_1$ and $\vx_2$ in Equation \@ref(eq:secondeq)). 

During the optimization procedure, `ivpml()` displays several messages which can be turned-off by setting `messages = FALSE`. The output indicates that the model is just-identified and that the initial values for the optimization procedure are obtained from the traditional probit and linear models for the structural and first-stage equation, respectively. Similarly to `hetprob()` function, the optimization algorithm can be managed using the argument `method`, which is passed on to the `maxLik()` function. Currently, the default algorithm is the Newton-Raphson, `method = "nr"`.

```{r summary-ivpml}
summary(fiml.probit)
```

During the optimization procedure the parameters $\sigma_{\upsilon}$ and $\rho$ might tend to the boundary points of the parameter space, generating identifiability problems of the MLE. To avoid this issue, `ivpml()` re-parametrizes the parameters.^[This re-parametrization is also used by `ivprobit` function in Stata.] First, to ensure $\sigma_{\upsilon} > 0$, `ivpml()` instead estimates $\log \nu_{\upsilon}$ such that:
\begin{equation}
\sigma_{\upsilon} = \exp(\log \nu_{\upsilon}). 
  (\#eq:repsigma)
\end{equation}

Second, `ivpml()` forces the correlation to remain in the $(-1, +1)$ range by using the inverse hyperbolic tangent:
\begin{equation*}
\textrm{atanh}(\rho) = \tau = \frac{1}{2}\log\left(\frac{1 +  \rho}{1 - \rho}\right),
\end{equation*}
where $\tau$ is unrestricted, and $\rho$ can be obtained using the inverse of $\tau$:
\begin{equation}
\tau ^{-1} = \rho = \tanh(\tau).
  (\#eq:reprho)
\end{equation}

In the following syntax, we recover $\sigma_{\upsilon}$ and $\rho$ using Equations \@ref(eq:repsigma) and \@ref(eq:reprho), respectively, and their standard errors are computed using delta method approach by `deltaMethod()` function: 

```{r rho-sigma}
deltaMethod(fiml.probit, "exp(lnsigma)")
deltaMethod(fiml.probit, "tanh(atanhrho)")
```

Again, the FIML estimate of $\rho$ is close to that found using the CF approach which was `0.279`. If significant, a positive $\rho$ would indicate that there is a positive correlation between $\epsilon$ and $\upsilon$. That is, the unobserved factors that make it more likely for a woman to have a higher income from other sources also make it more likely that the woman will be participating in the labor force.

For those users who are more familiar with Stata (see **Appendix D**), it is important to mention that its function `ivprobit` estimates the 95\%-CI for $\widehat{\rho}$ and $\widehat{\sigma}_{\upsilon}$ as follows:

```{r rho-sigma-stata}
cbind(exp(coef(fiml.probit)["lnsigma"] - qnorm(0.975) * stdEr(fiml.probit)["lnsigma"]), 
      exp(coef(fiml.probit)["lnsigma"] + qnorm(0.975) * stdEr(fiml.probit)["lnsigma"]))
cbind(tanh(coef(fiml.probit)["atanhrho"] - qnorm(0.975) * stdEr(fiml.probit)["atanhrho"]), 
      tanh(coef(fiml.probit)["atanhrho"] + qnorm(0.975) * stdEr(fiml.probit)["atanhrho"]))
```

The APEs can be estimated using the function `effect()`. The main argument of this function is `asf`. If `asf = TRUE` (the default), then the APEs are computed using Equation \@ref(eq:ME1y2ivprobit). On the other hand, if `asf = FALSE` the APEs are computed using Equation \@ref(eq:ME2y2ivprobit). 

```{r effect.ivpml}
summary(effect(fiml.probit))
summary(effect(fiml.probit, asf = FALSE))
```

The results show that both APEs are close to each other. Note also that the estimated APE for `nwifeinc` using the CF approach is very similar to that ones obtained by MLE. **Appendix D** also shows that the Stata function `ivprobit()` provides the same estimates and marginal effects as `ivpml()` function.

# Summary

The aim of the article was to provide a primer on estimating heteroskedastic and IV model for binary outcomes in R.  I also show that the current version of \CRANpkg{Rchoice} package (available at \url{https://cran.r-project.org/web/packages/Rchoice/index.html}) allows to estimate such models in a flexible way and provides accurate average marginal effects that are very similar to those provided by Stata's \code{margins} command. \CRANpkg{Rchoice} can be used in concert with other packages. For example, one can format the summary output from \CRANpkg{Rchoice} with \CRANpkg{memisc} to produce well-formatted tables for regression estimates

# Appendix A: Gradient and Hessian for binary response models with heteroskedasticity

In this section, I provide the analytic gradient and Hessian used by `hetprob()` function in \CRANpkg{Rchoice}. The log-likelihood function for the binary choice model with exponential heteroskedasticity can be written as:
\begin{equation*}
\ell(\vtheta)= \sum_{i = 1}^n\ln F(a_i),
\end{equation*}
where $F(\cdot)$ is either the CDF of the standard normal or standard logistic distribution, $\vtheta = \left(\vbeta^\top, \vdelta^\top\right)^\top$ is the full $(k+p)$-dimensional vector of parameters, and:
\begin{equation*}
\begin{aligned}
a_i & = q_i\left(\frac{\vx_i^\top\vbeta}{\exp(\vz_i^\top\vdelta)}\right), \\
q_i & = 2(y_{i}-1).
\end{aligned}
\end{equation*}

The gradient is:
\begin{equation*}
\begin{aligned}
\frac{\partial \ell(\vtheta)}{\partial \vtheta} & = \sum_{i=1}^n\left[\frac{f(a_i)}{F(a_i)}\frac{\partial a_i}{\partial \vtheta}\right], \\
& = \sum_{i=1}^n\left[m(a_i)\vg_{i}\right],
\end{aligned}
\end{equation*}
where $m(\cdot)=f(\cdot)/F(\cdot)=\phi(\cdot)/\Phi(\cdot)$ for the probit model and $m(\cdot) = 1 - \Lambda(\cdot)$ for the logit model, and $\partial a_i/\partial \vtheta = \vg_i$ such that:
\begin{equation*}
\vg_{i}  = \begin{pmatrix}
                      \frac{\partial a_i}{\partial \vbeta} \\
                      \frac{\partial a_i}{\partial \vdelta}
                     \end{pmatrix}
                   = \frac{q_i}{\exp(\vz_i^\top\vdelta)}
                     \begin{pmatrix}
                      \vx_i \\
                      -\left(\vx_i^\top\vbeta\right)\vz_i
                     \end{pmatrix}.
\end{equation*}

The Hessian is given by:
\begin{equation*}
\begin{aligned}
\frac{\partial^2 \ell(\vtheta)}{\partial \vtheta \partial \vtheta^\top}& = \frac{\partial }{\partial \vtheta^\top}\left(\frac{\partial \ell(\vtheta)}{\partial \vtheta} \right), \\
& = \sum_{i = 1}^n\left[h(a_i)\vg_{i}\vg_{i}^\top + m(a_i)\mH_{i}\right],
\end{aligned}
\end{equation*}
where $h(a_i) = \partial m(a_i)/\partial a_i=-a_im(a_i)-m(a_i)^2$ and:
\begin{equation*}
\mH_i = \frac{\partial a_i}{\partial \vtheta \partial \vtheta^\top} = \begin{pmatrix}
\frac{\partial a_i}{\partial \vbeta \partial \vbeta^\top} & \frac{\partial a_i}{\partial \vbeta \partial \vdelta^\top} \\
\frac{\partial a_i}{\partial \vdelta \partial \vbeta^\top} &  \frac{\partial a_i}{\partial \vdelta \partial \vdelta^\top}
\end{pmatrix}
=
\begin{pmatrix}
  \mO & -\frac{q_i}{\exp(\vz_i^\top\vdelta)}\vx_i\vz_i^\top \\
  -\frac{q_i}{\exp(\vz_i^\top\vdelta)}\vz_i\vx_i^\top & \frac{q_i(\vx_i^\top\vbeta)}{\exp(\vz_i^\top\vdelta)}\vz_i\vz_i^\top
\end{pmatrix}.
\end{equation*}

# Appendix B: Gradient and Hessian for binary response models with endogeneity

In this section, I provide the analytic gradient and Hessian used by \code{ivpml} function in \CRANpkg{Rchoice}. The log-likelihood function can be written as:
\begin{equation*}
\ell(\vtheta) = \sum_{i = 1}^n\left[\ln\left(\Phi(a_i)\right)+\ln(1)-\ln\left(\sigma_{\upsilon}\right)+\ln\left[\phi(b_i)\right]\right],
\end{equation*}
where $\vtheta = \left(\vbeta^\top, \vdelta^\top, \sigma_{\upsilon}, \rho\right)^\top$ is an $(k + p + 2)$-dimensional vector and:
\begin{equation*}
\begin{aligned}
  a_i & =  q_i\left(\frac{\vx_i^\top\vbeta + \frac{\rho}{\sigma_{\upsilon}}\left(y_{2i}-\vz_i^\top\vdelta\right)}{\sqrt{1-\rho^2}}\right),\\
  b_i & = \frac{y_{2i}-\vz_i^\top\vdelta}{\sigma_{\upsilon}},\\
  q_i & = 2(y_i - 1),\\
  \sigma_{\upsilon} &= \exp(\ln \nu_{\upsilon}), \\
  \rho & = \text{tanh}(\tau). 
\end{aligned}
\end{equation*}

The first derivatives of the log-likelihood function are:
\begin{equation*}
\begin{aligned}
\frac{\partial \ell(\vtheta)}{\partial \vbeta} & = \sum_{i = 1}^n\left[m(a_i)\left(\frac{q_i}{\sqrt{1-\rho^2}}\right)\vx_i\right], \\
\frac{\partial \ell(\vtheta)}{\partial \vdelta} & = \sum_{i = 1}^n\left[-m(a_i)\left(\frac{q_i\left(\rho/\sigma_{\upsilon}\right)}{\sqrt{1-\rho^2}}\right)+b_i\left(\frac{1}{\sigma_{\upsilon}}\right)\right]\vz_i, \\
\frac{\partial \ell(\vtheta)}{\partial \ln \nu_{\upsilon}} & = \sum_{i = 1}^n\left[-m(a_i)\frac{q_i\rho}{\sqrt{1-\rho^2}}b_i+b_i^2-1\right], \\
\frac{\partial \ell(\vtheta)}{\partial \tau}
& = \sum_{i = 1}^n\left[m(a_i)q_i\left(\frac{\vx_i^\top\vbeta\rho +    b_i}{\sqrt{\text{sech}^2(\tau)}}\right)\right],
\end{aligned}
\end{equation*}
where $m(a_i)=\phi(a_i)/\Phi(a_i)$, $d\text{tanh}(\tau)/d\tau = \text{sech}^2(\tau)$, and we use the fact that $\phi'(b_i)= -b_i\phi(b_i)$ so that $\phi'(b_i)/\phi(b_i)=-b_i$.

The Hessian is given by:
\begin{equation*}
\mH = \begin{pmatrix}
         \frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \vbeta^\top} & \frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \vdelta^\top} & \frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \ln \nu_{\upsilon}} & \frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \tau} \\
         \dot &  \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \vdelta^\top} &   \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \partial \ln \nu_{\upsilon}} & \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \tau} \\
         \dot & \dot & \frac{\partial^2 \ell(\vtheta)}{\partial (\ln \nu_{\upsilon})^2} & \frac{\partial^2 \ell(\vtheta)}{\partial \ln \nu_{\upsilon} \partial \tau} \\
         \dot & \dot & \dot & \frac{\partial^2 \ell(\vtheta)}{\partial \tau^2}
      \end{pmatrix}.
\end{equation*}

The second derivatives are: 
\begin{equation*}
\begin{aligned}
\frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \vbeta^\top} & = \sum_{i = 1}^n\left[h(a_i)\left(\frac{q_i}{\sqrt{1-\rho^2}}\right)^2\vx_i\vx_i^\top\right],  \\
\frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \vdelta^\top} & =\sum_{i = 1}^n\left[-h(a_i)\left(\frac{q_i}{\sqrt{1-\rho^2}}\right)^2\left(\frac{\rho}{\sigma_{\upsilon}}\right)\vx_i\vz_i^\top\right],  \\
\frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \ln \nu_{\upsilon}} & =\sum_{i = 1}^n\left[-h(a_i)\left(\frac{q_i}{\sqrt{1-\rho^2}}\right)^2\left(\rho b_i\right)\vx_i\right],  \\
\frac{\partial^2 \ell(\vtheta)}{\partial \vbeta \partial \tau} & =\sum_{i = 1}^n\left[h(a_i)\left(\frac{q_i}{\sqrt{1-\rho^2}}\right)\left(q_i\left(\frac{\vx_i^\top\vbeta\rho +b_i}{\sqrt{\text{sech}^2(\tau)}}\right)\right)\vx_i\right], \\
  \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \vdelta^\top} & = \sum_{i = 1}^n\left[h(a_i)\left(\frac{q_i\left(\rho/\sigma_{\upsilon}\right)}{\sqrt{1-\rho^2}}\right)^2-\frac{1}{\sigma^2_{\upsilon}}\right]\vz_i\vz_i^\top, \\
  \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \ln \nu_{\upsilon}} & =\sum_{i = 1}^n\left(\frac{b_i}{\sigma_{\upsilon}}\right)\left[h(a_i)\left(\frac{q_i\rho}{\sqrt{1-\rho^2}}\right)^2-2\right]\vz_i, \\
    \frac{\partial^2 \ell(\vtheta)}{\partial \vdelta \partial \tau} & =\sum_{i = 1}^n\left[-h(a_i)\left(\frac{q_i\left(\rho/\sigma_{\upsilon}\right)}{\sqrt{1-\rho^2}}\right)\left(q_i\left(\frac{\vx_i^\top\vbeta\rho +b_i}{\sqrt{\text{sech}^2(\tau)}}\right)\right)\right]\vz_i, \\
\frac{\partial^2 \ell(\vtheta)}{\partial (\ln \nu_{\upsilon})^2}& = \sum_{i = 1}^n\left[h(a_i)\left(\frac{q_i\rho b_i}{\sqrt{1-\rho^2}}\right)^2+m(a_i)\left(\frac{q_i\rho b_i}{\sqrt{1-\rho^2}}\right)-2b_i^2\right], \\
\frac{\partial^2 \ell(\vtheta)}{\partial \ln \nu_{\upsilon} \partial \tau} & = \sum_{i = 1}^n\left\lbrace-b_i\left[h(a_i)\frac{q_i\rho}{\sqrt{1-\rho^2}}q_i\left(\frac{\vx_i^\top\vbeta\rho +    b_i}{\sqrt{\text{sech}^2(\tau)}}\right)
+ m(a_i)\frac{q_i}{\sqrt{\text{sech}^2(\tau)}}
\right]\right\rbrace, \\
\frac{\partial^2 \ell(\vtheta)}{\partial \tau^2} & = \sum_{i = 1}^n\left[h(a_i)\left(q_i\frac{\vx_i^\top\vbeta\rho +    b_i}{\sqrt{\text{sech}^2(\tau)}}\right)^2+q_i m(a_i)\frac{\vx_i^\top\vbeta + b_i\rho}{\sqrt{\text{sech}^2(\tau)}}\right],
\end{aligned}
\end{equation*}
where $h(a_i) = - a_i m(a_i)- m(a_i)^2$.

# Appendix C: Stata code for heteroskedastic binary response models

````{verbatim}
 *=========================================
 *** Example 1: Promotion of scientists ***
 *=========================================
. import delimited "$dir/tenure.csv", clear 
(23 vars, 2,945 obs)

. ** Logit models for men and women
. quietly eststo logit_m: logit tenure year c.year#c.year select ///
                          articles prestige if (year <= 10 & female == 0)

. quietly eststo logit_w: logit tenure year c.year#c.year select ///
                          articles prestige if (year <= 10 & female == 1)

. esttab logit_m logit_w, b(3) se(3)
--------------------------------------------
                      (1)             (2)   
                   tenure          tenure   
--------------------------------------------
tenure                                      
year                1.909***        1.408***
                  (0.214)         (0.257)   
c.year#c.y~r       -0.143***       -0.096***
                  (0.019)         (0.022)   
select              0.216***        0.055   
                  (0.061)         (0.072)   
articles            0.074***        0.034** 
                  (0.012)         (0.013)   
prestige           -0.431***       -0.371*  
                  (0.109)         (0.156)   
_cons              -7.680***       -5.842***
                  (0.681)         (0.866)   
--------------------------------------------
N                    1741            1056   
--------------------------------------------
Standard errors in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. ** Heterokedastic logit model
. quietly ssc install oglm

. quietly eststo het_logit: oglm tenure i.female year c.year#c.year select ///
                           articles prestige if (year <= 10), hetero(i.female) link(logit)

. esttab logit_m logit_w het_logit, b(3) se(3)
------------------------------------------------------------
                      (1)             (2)             (3)   
                   tenure          tenure          tenure   
------------------------------------------------------------
tenure                                                      
year                1.909***        1.408***        1.910***
                  (0.214)         (0.257)         (0.200)   
c.year#c.y~r       -0.143***       -0.096***       -0.140***
                  (0.019)         (0.022)         (0.017)   
select              0.216***        0.055           0.182***
                  (0.061)         (0.072)         (0.053)   
articles            0.074***        0.034**         0.064***
                  (0.012)         (0.013)         (0.010)   
prestige           -0.431***       -0.371*         -0.446***
                  (0.109)         (0.156)         (0.097)   
0.female                                            0.000   
                                                      (.)   
1.female                                           -0.939*  
                                                  (0.371)   
_cons              -7.680***       -5.842***                
                  (0.681)         (0.866)                   
------------------------------------------------------------
lnsigma                                                     
0.female                                            0.000   
                                                      (.)   
1.female                                            0.302*  
                                                  (0.146)   
------------------------------------------------------------
cut1                                                        
_cons                                               7.491***
                                                  (0.660)   
------------------------------------------------------------
N                    1741            1056            2797   
------------------------------------------------------------
Standard errors in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. ** Testing how much the disturbance standard deviation differ by gender
. margins, expression((1 - exp([lnsigma]_b[1.female])) / exp([lnsigma]_b[1.female]))
Warning: expression() does not contain predict() or xb().
Warning: prediction constant over observations.

Predictive margins                              Number of obs     =      2,797
Model VCE    : OIM
Expression   : (1 - exp([lnsigma]_b[1.female])) / exp([lnsigma]_b[1.female])

------------------------------------------------------------------------------
             |            Delta-method
             |     Margin   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       _cons |  -.2608323   .1080501    -2.41   0.016    -.4726065   -.0490581
------------------------------------------------------------------------------

. ** Heterokedastic logit model 2
. eststo het_logit2: oglm tenure i.female year c.year#c.year select ///
            articles prestige i.female#c.articles if (year <= 10), hetero(i.female) link(logit)

Heteroskedastic Ordered Logistic Regression     Number of obs     =      2,797
                                                LR chi2(8)        =     415.39
                                                Prob > chi2       =     0.0000
Log likelihood = -835.13347                     Pseudo R2         =     0.1992

-----------------------------------------------------------------------------------
           tenure |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
------------------+----------------------------------------------------------------
tenure            |
         1.female |  -.3780598   .4500207    -0.84   0.401    -1.260084    .5039645
             year |   1.838257   .2029491     9.06   0.000     1.440484     2.23603
                  |
    c.year#c.year |  -.1342828    .017024    -7.89   0.000    -.1676492   -.1009165
                  |
           select |   .1699659   .0516643     3.29   0.001     .0687057    .2712261
         articles |   .0719821   .0114106     6.31   0.000     .0496178    .0943464
         prestige |  -.4204742   .0961206    -4.37   0.000     -.608867   -.2320813
                  |
female#c.articles |
               1  |  -.0304836   .0187427    -1.63   0.104    -.0672185    .0062514
------------------+----------------------------------------------------------------
lnsigma           |
         1.female |   .1774193   .1627087     1.09   0.276    -.1414839    .4963226
------------------+----------------------------------------------------------------
            /cut1 |   7.365286   .6547121    11.25   0.000     6.082074    8.648498
-----------------------------------------------------------------------------------

. ** Plot predicted probability and sigma
. predict phat, pr outcome(1)
. predict sigmahat, sigma

. hist phat
(bin=34, start=2.232e-12, width=.02503351)

. hist sigmahat
(bin=34, start=1, width=.00570976)

. ** Average Marginal Effects for logit and probit heterokedastic models
. quietly oglm tenure i.female year c.year#c.year select ///
                articles prestige i.female#c.articles if (year <= 10), hetero(i.female) link(probit)

. eststo eff_probit: margins, dydx(*) predict(outcome(1)) post
Average marginal effects                        Number of obs     =      2,797
Model VCE    : OIM
Expression   : Pr(tenure==1), predict(outcome(1))
dy/dx w.r.t. : 1.female year select articles prestige
------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |   -.031161   .0115614    -2.70   0.007     -.053821   -.0085011
        year |    .031839   .0019586    16.26   0.000     .0280002    .0356779
      select |   .0142546   .0041796     3.41   0.001     .0060626    .0224465
    articles |     .00559   .0007685     7.27   0.000     .0040838    .0070962
    prestige |  -.0350608   .0077056    -4.55   0.000    -.0501635   -.0199581
------------------------------------------------------------------------------
Note: dy/dx for factor levels is the discrete change from the base level.

. quietly oglm tenure i.female year c.year#c.year select ///
          articles prestige i.female#c.articles if (year <= 10), hetero(i.female) link(logit)

. eststo eff_logit: margins, dydx(*) predict(outcome(1)) post
Average marginal effects                        Number of obs     =      2,797
Model VCE    : OIM
Expression   : Pr(tenure==1), predict(outcome(1))
dy/dx w.r.t. : 1.female year select articles prestige
------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    1.female |  -.0312105   .0115836    -2.69   0.007     -.053914    -.008507
        year |   .0319057   .0019277    16.55   0.000     .0281275    .0356839
      select |   .0145808   .0042388     3.44   0.001      .006273    .0228886
    articles |   .0053378   .0007523     7.09   0.000     .0038632    .0068123
    prestige |  -.0360711    .007934    -4.55   0.000    -.0516215   -.0205207
------------------------------------------------------------------------------
Note: dy/dx for factor levels is the discrete change from the base level.

. esttab eff_probit eff_logit, b(3) se(3)
--------------------------------------------
                      (1)             (2)   
                                            
--------------------------------------------
0.female            0.000           0.000   
                      (.)             (.)   
1.female           -0.031**        -0.031** 
                  (0.012)         (0.012)   
year                0.032***        0.032***
                  (0.002)         (0.002)   
select              0.014***        0.015***
                  (0.004)         (0.004)   
articles            0.006***        0.005***
                  (0.001)         (0.001)   
prestige           -0.035***       -0.036***
                  (0.008)         (0.008)   
--------------------------------------------
N                    2797            2797   
--------------------------------------------
Standard errors in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. *=========================================
. *** Example 2: Labor Participation ***
. *=========================================
. 
. * Open dataset and create variables
. import delimited "$dir/mroz.csv", clear 
. gen kids = (kidslt6 + kidsge6) > 0
. gen finc = faminc/10000

. * Hetekedastic binary probit model
. quietly eststo labor_hom: probit inlf age c.age#c.age finc educ kids
. quietly eststo labor_het: oglm inlf age c.age#c.age finc educ i.kids, ///
                            hetero(finc i.kids) link(probit)
. quietly eststo eff_labor_het: margins, dydx(*) predict(outcome(1)) post
. esttab labor_hom labor_het eff_labor_het, b(3) se(3)

------------------------------------------------------------
                      (1)             (2)             (3)   
                     inlf            inlf                   
------------------------------------------------------------
main                                                        
age                 0.185**         0.264*         -0.009***
                  (0.066)         (0.118)         (0.003)   
c.age#c.age        -0.002**        -0.004*                  
                  (0.001)         (0.001)                   
finc                0.046           0.424           0.069** 
                  (0.042)         (0.222)         (0.024)   
educ                0.098***        0.140**         0.030***
                  (0.023)         (0.052)         (0.009)   
kids               -0.449***                                
                  (0.131)                                   
0.kids                              0.000           0.000   
                                      (.)             (.)   
1.kids                             -0.879**        -0.161***
                                  (0.303)         (0.043)   
_cons              -4.157**                                 
                  (1.402)                                   
------------------------------------------------------------
lnsigma                                                     
finc                                0.313*                  
                                  (0.123)                   
0.kids                              0.000                   
                                      (.)                   
1.kids                             -0.141                   
                                  (0.324)                   
------------------------------------------------------------
cut1                                                        
_cons                               6.030*                  
                                  (2.498)                   
------------------------------------------------------------
N                     753             753             753   
------------------------------------------------------------
Standard errors in parentheses
* p<0.05, ** p<0.01, *** p<0.001

. * Wald test
. estimates restore labor_het
(results labor_het are active now)

. quietly oglm
. test [lnsigma]: finc 1.kids

 ( 1)  [lnsigma]finc = 0
 ( 2)  [lnsigma]1.kids = 0

           chi2(  2) =    6.53
         Prob > chi2 =    0.0381

````
# Appendix D: Stata code for binary response models with endogeneity

````{verbatim}
. ************************************
. *** IV Probit ***
. ************************************

. *=========================================
. *** Control function approach ***
. *=========================================

. import delimited "$dir/mroz.csv", clear 
(22 vars, 753 obs)

. * Probit estimates and marginal effect 
. probit inlf educ exper c.exper#c.exper age kidslt6 kidsge6 nwifeinc

Iteration 0:   log likelihood =  -514.8732  
Iteration 1:   log likelihood = -402.06651  
Iteration 2:   log likelihood = -401.30273  
Iteration 3:   log likelihood = -401.30219  
Iteration 4:   log likelihood = -401.30219  

Probit regression                               Number of obs     =        753
                                                LR chi2(7)        =     227.14
                                                Prob > chi2       =     0.0000
Log likelihood = -401.30219                     Pseudo R2         =     0.2206
---------------------------------------------------------------------------------
           inlf |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
----------------+----------------------------------------------------------------
           educ |   .1309047   .0252542     5.18   0.000     .0814074     .180402
          exper |   .1233476   .0187164     6.59   0.000     .0866641    .1600311
                |
c.exper#c.exper |  -.0018871      .0006    -3.15   0.002     -.003063   -.0007111
                |
            age |  -.0528527   .0084772    -6.23   0.000    -.0694678   -.0362376
        kidslt6 |  -.8683285   .1185223    -7.33   0.000    -1.100628    -.636029
        kidsge6 |    .036005   .0434768     0.83   0.408     -.049208    .1212179
       nwifeinc |  -.0120237   .0048398    -2.48   0.013    -.0215096   -.0025378
          _cons |   .2700768    .508593     0.53   0.595    -.7267473    1.266901
---------------------------------------------------------------------------------

. margins, dydx(nwifeinc)
Average marginal effects                        Number of obs     =        753
Model VCE    : OIM
Expression   : Pr(inlf), predict()
dy/dx w.r.t. : nwifeinc

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    nwifeinc |  -.0036162   .0014414    -2.51   0.012    -.0064413   -.0007911
------------------------------------------------------------------------------

. * Control function approach
. eststo fstep: reg nwifeinc educ exper c.exper#c.exper age kidslt6 kidsge6 huseduc

      Source |       SS           df       MS      Number of obs   =       753
-------------+----------------------------------   F(7, 745)       =     27.13
       Model |  20676.7705         7  2953.82436   Prob > F        =    0.0000
    Residual |  81120.3451       745  108.886369   R-squared       =    0.2031
-------------+----------------------------------   Adj R-squared   =    0.1956
       Total |  101797.116       752  135.368505   Root MSE        =    10.435
---------------------------------------------------------------------------------
       nwifeinc |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
----------------+----------------------------------------------------------------
           educ |   .6746951   .2136829     3.16   0.002     .2552029    1.094187
          exper |  -.3129877   .1382549    -2.26   0.024    -.5844034   -.0415721
                |
c.exper#c.exper |  -.0004776   .0045196    -0.11   0.916    -.0093501     .008395
                |
            age |   .3401521   .0597084     5.70   0.000     .2229354    .4573687
        kidslt6 |   .8262719   .8183785     1.01   0.313    -.7803305    2.432874
        kidsge6 |   .4355289   .3219888     1.35   0.177    -.1965845    1.067642
        huseduc |   1.178155   .1609449     7.32   0.000     .8621956    1.494115
          _cons |  -14.72048   3.787326    -3.89   0.000    -22.15559   -7.285383
---------------------------------------------------------------------------------

. predict res_hat, resi

. test huseduc
 ( 1)  huseduc = 0
       F(  1,   745) =   53.59
            Prob > F =    0.0000

. eststo sstep: probit inlf educ exper c.exper#c.exper age kidslt6 kidsge6 nwifeinc res_hat

Iteration 0:   log likelihood =  -514.8732  
Iteration 1:   log likelihood = -401.13728  
Iteration 2:   log likelihood = -400.30361  
Iteration 3:   log likelihood = -400.30301  
Iteration 4:   log likelihood = -400.30301  

Probit regression                               Number of obs     =        753
                                                LR chi2(8)        =     229.14
                                                Prob > chi2       =     0.0000
Log likelihood = -400.30301                     Pseudo R2         =     0.2225
---------------------------------------------------------------------------------
           inlf |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
----------------+----------------------------------------------------------------
           educ |   .1702153   .0376718     4.52   0.000     .0963798    .2440507
          exper |   .1163123   .0193312     6.02   0.000     .0784239    .1542007
                |
c.exper#c.exper |  -.0019459   .0006009    -3.24   0.001    -.0031235   -.0007682
                |
            age |   -.044953   .0101367    -4.43   0.000    -.0648206   -.0250855
        kidslt6 |  -.8444363   .1198154    -7.05   0.000     -1.07927   -.6096025
        kidsge6 |   .0477905   .0443204     1.08   0.281    -.0390758    .1346568
       nwifeinc |  -.0368641   .0182706    -2.02   0.044    -.0726738   -.0010543
        res_hat |   .0267093   .0189352     1.41   0.158    -.0104031    .0638217
          _cons |   .0171187   .5392914     0.03   0.975    -1.039873     1.07411
---------------------------------------------------------------------------------

. * Two-step IV-probit
. ivprobit inlf educ exper c.exper#c.exper age kidslt6 kidsge6 (nwifeinc = huseduc), twostep
Checking reduced-form model...

Two-step probit with endogenous regressors        Number of obs   =        753
                                                  Wald chi2(7)    =     173.79
                                                  Prob > chi2     =     0.0000
---------------------------------------------------------------------------------
                |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
----------------+----------------------------------------------------------------
       nwifeinc |  -.0368641   .0186314    -1.98   0.048    -.0733809   -.0003472
           educ |   .1702153   .0384014     4.43   0.000     .0949499    .2454806
          exper |   .1163123   .0197084     5.90   0.000     .0776846      .15494
                |
c.exper#c.exper |  -.0019459   .0006129    -3.17   0.001    -.0031471   -.0007446
                |
            age |   -.044953    .010327    -4.35   0.000    -.0651936   -.0247125
        kidslt6 |  -.8444363   .1218529    -6.93   0.000    -1.083264    -.605609
        kidsge6 |   .0477905    .045177     1.06   0.290    -.0407549    .1363359
          _cons |   .0171187   .5498911     0.03   0.975    -1.060648    1.094885
---------------------------------------------------------------------------------
Instrumented:  nwifeinc
Instruments:   educ exper c.exper#c.exper age kidslt6 kidsge6 huseduc
---------------------------------------------------------------------------------
Wald test of exogeneity: chi2(1) = 1.99                   Prob > chi2 = 0.1584


. *=========================================
. *** MLE ***
. *=========================================

. ivprobit inlf educ exper c.exper#c.exper age kidslt6 kidsge6 (nwifeinc = huseduc)

Fitting exogenous probit model

Iteration 0:   log likelihood =  -514.8732  
Iteration 1:   log likelihood = -401.13728  
Iteration 2:   log likelihood = -400.30361  
Iteration 3:   log likelihood = -400.30301  
Iteration 4:   log likelihood = -400.30301  

Fitting full model

Iteration 0:   log likelihood = -3230.6635  
Iteration 1:   log likelihood = -3230.6421  
Iteration 2:   log likelihood = -3230.6421  

Probit model with endogenous regressors         Number of obs     =        753
                                                Wald chi2(7)      =     200.50
Log likelihood = -3230.6421                     Prob > chi2       =     0.0000
-----------------------------------------------------------------------------------------
                        |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
------------------------+----------------------------------------------------------------
               nwifeinc |  -.0355243   .0161904    -2.19   0.028    -.0672569   -.0037916
                   educ |   .1640289   .0312249     5.25   0.000     .1028293    .2252285
                  exper |    .112085   .0211991     5.29   0.000     .0705356    .1536344
                        |
        c.exper#c.exper |  -.0018751   .0005915    -3.17   0.002    -.0030345   -.0007158
                        |
                    age |  -.0433193   .0113314    -3.82   0.000    -.0655284   -.0211101
                kidslt6 |  -.8137458   .1299442    -6.26   0.000    -1.068432   -.5590599
                kidsge6 |   .0460536   .0431386     1.07   0.286    -.0384966    .1306037
                  _cons |   .0164965   .5300821     0.03   0.975    -1.022445    1.055438
------------------------+----------------------------------------------------------------
 corr(e.nwifeinc,e.inlf)|   .2671475   .1791903                     -.1040303    .5730063
          sd(e.nwifeinc)|   10.37928   .2674576                      9.868095    10.91695
-----------------------------------------------------------------------------------------
Instrumented:  nwifeinc
Instruments:   educ exper c.exper#c.exper age kidslt6 kidsge6 huseduc
-----------------------------------------------------------------------------------------
Wald test of exogeneity (corr = 0): chi2(1) = 2.01        Prob > chi2 = 0.1559

. eststo me1: margins, dydx(*) predict(pr) post
Average marginal effects                        Number of obs     =        753
Model VCE    : OIM
Expression   : Average structural function probabilities, predict(pr)
dy/dx w.r.t. : nwifeinc educ exper age kidslt6 kidsge6
------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    nwifeinc |  -.0110576   .0055497    -1.99   0.046    -.0219348   -.0001805
        educ |   .0510572   .0111011     4.60   0.000     .0292994     .072815
       exper |   .0230711   .0029517     7.82   0.000     .0172858    .0288563
         age |   -.013484    .002986    -4.52   0.000    -.0193365   -.0076314
     kidslt6 |  -.2532945   .0330766    -7.66   0.000    -.3181235   -.1884655
     kidsge6 |   .0143351   .0135204     1.06   0.289    -.0121644    .0408345
------------------------------------------------------------------------------

. qui ivprobit inlf educ exper c.exper#c.exper age kidslt6 kidsge6 (nwifeinc = huseduc)
. eststo me2: margins, dydx(*) predict(pr fix(nwifeinc)) post

Average marginal effects                        Number of obs     =        753
Model VCE    : OIM

Expression   : Average structural function probabilities, predict(pr fix(nwifeinc))
dy/dx w.r.t. : nwifeinc educ exper age kidslt6 kidsge6
------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    nwifeinc |  -.0105638   .0047364    -2.23   0.026    -.0198469   -.0012807
        educ |   .0487769   .0087333     5.59   0.000       .03166    .0658937
       exper |   .0219965   .0037232     5.91   0.000     .0146992    .0292939
         age |  -.0128817   .0033216    -3.88   0.000     -.019392   -.0063714
     kidslt6 |  -.2419815   .0365941    -6.61   0.000    -.3137047   -.1702583
     kidsge6 |   .0136948   .0127924     1.07   0.284    -.0113777    .0387674
------------------------------------------------------------------------------
Warning: The chosen prediction can result in estimates of derivatives or
         contrasts that do not have a structural function interpretation.

. esttab  me1 me2, b(3) se(3)
--------------------------------------------
                      (1)             (2)   
                                            
--------------------------------------------
nwifeinc           -0.011*         -0.011*  
                  (0.006)         (0.005)   
educ                0.051***        0.049***
                  (0.011)         (0.009)   
exper               0.023***        0.022***
                  (0.003)         (0.004)   
age                -0.013***       -0.013***
                  (0.003)         (0.003)   
kidslt6            -0.253***       -0.242***
                  (0.033)         (0.037)   
kidsge6             0.014           0.014   
                  (0.014)         (0.013)   
--------------------------------------------
N                     753             753   
--------------------------------------------
Standard errors in parentheses
* p<0.05, ** p<0.01, *** p<0.001
````

# Acknowledgments

I express my thanks to FONDECYT, Chile, Grant 1200522 for full financial support. 
