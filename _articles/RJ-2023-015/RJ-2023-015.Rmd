---
title: Bootstrapping Clustered Data in R using lmeresampler
author:
- name: Adam Loy
  affiliation: Carleton College
  address: Northfield, MN, USA
  url: https://aloy.rbind.io/
  orcid: 0000-0002-5780-4611
  email: aloy@carleton.edu
- name: Jenna Korobova
  email: jenna.korobova@gmail.com
  affiliation: Carleton College
  address: Northfield, MN, USA
abstract: |
  Linear mixed-effects models are commonly used to analyze clustered data structures. There are numerous packages to fit these models in R and conduct likelihood-based inference. The implementation of resampling-based procedures for inference are more  limited. In this paper, we introduce the \pkg{lmeresampler} package for bootstrapping nested linear mixed-effects models fit via \pkg{lme4} or \pkg{nlme}. Bootstrap  estimation allows for bias correction, adjusted standard errors and confidence  intervals for small samples sizes and when distributional assumptions break down.  We will also illustrate how bootstrap resampling can be used to diagnose this model  class. In addition, \pkg{lmeresampler} makes it easy to construct interval estimates  of functions of model parameters.
preamble: |
  % Any extra LaTeX you need in the preamble
  \usepackage{bm}
  \usepackage{booktabs}
  \usepackage{longtable}
bibliography: loy.bib
output:
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
editor_options:
  markdown:
    wrap: 72
date: '2023-02-10'
date_received: '2021-05-10'
volume: 14
issue: 4
slug: RJ-2023-015
draft: no
journal:
  lastpage: 120
  firstpage: 103

---



```{r setup, include=FALSE}
library(lmeresampler)
library(HLMdiag)
library(lme4)
library(nlme)
```


## Introduction

Clustered data structures occur in a wide range of studies. For example, students are organized within classrooms, schools, and districts, imposing a correlation structure that must be accounted for in the modeling process. Similarly, cholesterol measurements could be tracked across time for a number of subjects, resulting in measurements being grouped by subject. Other names for clustered data structures include grouped, nested, multilevel, hierarchical, longitudinal, repeated measurements, and blocked data. The covariance structure imposed by clustered data is commonly modeled using linear mixed-effects (LME) models, also referred to as hierarchical linear or multilevel linear models [@Pinhiero:2000vf; @raudenbush; @goldstein2011]. 

In this paper, we restrict attention to the Gaussian response LME model for clustered data structures. For cluster
$i=1, \ldots, g$, this model is expressed as 
\begin{equation}
    \underset{(n_i \times 1)}{\boldsymbol{y}_i} = \underset{(n_i \times p)}{\boldsymbol{X}_i} \ \underset{(p \times 1)}{\boldsymbol{\beta}} + \underset{(n_i \times q)}{\boldsymbol{Z}_i} \ \underset{(q \times 1)}{\boldsymbol{b}_i} + \underset{(n_i \times 1)}{\boldsymbol{\varepsilon}_i},
    (\#eq:lme)
\end{equation} 
where
\begin{equation}
\boldsymbol{\varepsilon}_i \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_{n_i}),   \boldsymbol{b}_i \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{D}),
(\#eq:dsns)
\end{equation} 
where $\boldsymbol{\varepsilon}_i$ and $\boldsymbol{b}_i$ are independent for all $i$, $\boldsymbol{\varepsilon}_i$ is independent of $\boldsymbol{\varepsilon}_j$ for $i\ne j$, and $\boldsymbol{b}_i$ is independent of $\boldsymbol{b}_j$ for $i\ne j$. Here, $\boldsymbol{0}$ denotes a vector of zeros of length $n_i$ (the number of observations in group $i$), $\boldsymbol{I}_{n_i}$ denotes the $n_i$-dimensional identity matrix,
and $\boldsymbol{D}$ is a $q \times q$ covariance matrix.

In R, the two most popular packages to fit LME models are \CRANpkg{nlme} [@nlme] and \CRANpkg{lme4} [@lme4]. Both packages fit LME models using either maximum likelihood or restricted maximum likelihood methods. These methods rely on the distributional assumptions placed on the residual quantities, $\boldsymbol{\varepsilon}_i$ and $\boldsymbol{b}_i$, as well as large enough sample sizes. While some aspects of LME models are quite robust to model misspecification, others are more sensitive, leading to biased estimates and/or incorrect standard errors. @Jacqmin-Gadda2007-ll found that inference for the fixed effects is robust if the distribution of the error terms, $\boldsymbol{\varepsilon}_i$, is non-normal or heteroscedastic, but that  variance components and random effects are biased if the covariance structure for the error terms is misspecified. The fixed intercept is not robust to misspecification of the random effects [@Hui2021-cx]. In addition, misspecification of the random effects distribution can lead to biased estimates of the variance components and undercoverage of the confidence intervals [@Hui2021-cx]. In cases where distributional assumptions are suspect, bootstrapping provides an alternative inferential approach that leads to consistent, bias-corrected parameter estimates, standard errors, and confidence intervals. Standard errors and confidence intervals for functions of model parameters are also easily calculated using a bootstrap procedure, and are available even in situations where closed-form solutions are not.

A variety of bootstrap procedures for clustered data and the LME model have been proposed and investigated, including the cases (non-parametric) bootstrap, the residual bootstrap, the parametric bootstrap, the random effect block (REB) bootstrap, and the wild bootstrap [@Morris:2002tj; @Carpenter:2003uy; @Field:2007vm; @Chambers:2013ba; @Modugno2015-kd].  @VanderLeeden:2008 provide a thorough overview of bootstrapping LME models. @Sanchez-Espigares2009-yg developed a full-featured bootstrapping framework for LME models in R; however, this package is not available and there appears to be no active development. Consequently, R users must pick and choose packages based on what bootstrap procedure they wish to implement. The parametric bootstrap is available  in \pkg{lme4} via the `bootMer()` function, as is the semi-parametric (residual) bootstrap proposed by @Morris:2002tj. The `simulateY()` function in  \pkg{nlmeU} [@galecki2013] makes it easy to simulate values of the response variable for `lme` model objects; however, the user is required to implement the remainder of the parametric bootstrap.  @Chambers:2013ba made R code available to implement their REB bootstrap as well as the parametric bootstrap and the residual bootstrap proposed by @Carpenter:2003uy for LME models fix via `nlme::lme()`. Unfortunately, these functions were not published on CRAN or extended to models fit via `lme4::lmer()`. Bootstrap procedures for specific inferential tasks have also been implemented. The parametric bootstrap has been implemented to carry out likelihood ratio tests for the presence of variance components in \CRANpkg{RLRsim} [@rlrsim]. \CRANpkg{pbkrtest} [@pbkrtest] provides a Kenward-Roger approximation for performing F-tests using a parametric bootstrap approach for LME models and generalized LME models. \CRANpkg{rptR} uses the parametric bootstrap to estimate repeatabilities for LME models [@rptR]. Finally, constrained inference for LMEs using the residual bootstrap is implemented in \CRANpkg{CLME} [@clme]. 

<!-- @VanderLeeden:2008 provide a thorough overview of bootstrapping LME models and note that these procedures are not widely available in R. Consequently, R users either need to leave the R ecosystem and use a specialty software such as MLwiN [@Rasbash:2012] to run these procedures, or need to spend time programming these bootstrap procedures. Recently, some progress has been made in this area. @Sanchez-Espigares2009-yg developed a full-featured bootstrapping framework for LME models in R; however, this package is not available and there appears to be no active development.  The parametric bootstrap is now available  in \pkg{lme4} via the `bootMer()` function, and the `simulateY()` function in  \pkg{nlmeU} [@galecki2013] makes it easy to simulate values of the response variable for `lme` model objects, though the user is still required to implement the remainder of the parametric bootstrap. Bootstrap procedures for specific inferential tasks have also been implemented. Most notably, the parametric bootstrap has been implemented to carry out likelihood ratio tests for the presence of variance components in \CRANpkg{RLRsim} [@rlrsim] and to carry out tests for the mean structure in \CRANpkg{pbkrtest} [@pbkrtest]. \CRANpkg{rptR} uses the parametric bootstrap to estimate repeatabilities for LME models [@rptR]. -->

In this paper, we introduce the \CRANpkg{lmeresampler} package which implements a suite of bootstrap methods for LME models fit using either \pkg{nlme} or \pkg{lme4} in a single package. \pkg{lmeresampler} provides a unified `bootstrap()` command to reduce cognitive load on the user and also provides access to procedures that were not previously available on CRAN. In the next section, we will clarify some notation and terminology for LME models. In [Bootstrap procedures for clustered data], we provide an overview of the bootstrap methods implemented in \pkg{lmeresampler}. We then give an [Overview of lmeresampler], discuss a variety of [Example applications] for LME models, and show how to [Bootstrapping in parallel] works in \pkg{lmeresampler}. We conclude with a [Summary] and highlight areas for future development.




## Bootstrap procedures for clustered data

In \pkg{lmeresampler}, we implement five bootstrap procedures for clustered data: a cases
(i.e., nonparamteric) bootstrap, a residual bootstrap (i.e., semiparametric), a parametric
bootstrap, a wild bootstrap, and a block bootstrap. In this section, 
we provide an overview of these bootstrap approaches. Our discussion 
focuses on two-level models, but procedures generalize to 
higher-level models unless otherwise noted.


### The cases bootstrap

The cases bootstrap is a fully non-parametric bootstrap that resamples
the clusters in the data set to generate bootstrap
resamples. Depending on the nature of the data, this resampling can be
done only for the top-level cluster, only at the observation-level
within a cluster, or at both levels. The choice of the exact resampling
scheme should be dictated by the way the data were generated, since the
cases bootstrap generates new data by mimicking this process. 
@VanderLeeden:2008 provide a cogent explanation of how to select  a 
resampling scheme. To help ground the idea of resampling, consider a two-level 
hierarchical data set where students are organized into schools.

One version of the cases bootstrap is implemented by only
resampling the clusters. This version of the bootstrap is what @Field:2007vm term the 
cluster bootstrap and @goldstein2011 term the non-parametric bootstrap. We would choose this resampling scheme, for example, if schools were chosen at random and then all students within each school were observed. In this case, the bootstrap proceeds as follows:

1.  Draw a random sample, with replacement, of size $g$ from the
    clusters.
2.  For each selected cluster, $k$, extract all of the cases to form the
    bootstrap sample $\left(\boldsymbol{y}^*_k, \boldsymbol{X}^*_k, \boldsymbol{Z}^*_k \right)$.
    Since entire clusters are sampled, the total sample size may differ from the that of the original data set.
3.  Refit the model to the bootstrap sample and extract the parameter
    estimates of interest.
4.  Repeat steps 1--3 $B$ times.

\noindent An alternative version of the cases bootstrap only resamples the
observations within clusters, which makes sense in our example 
if the schools were fixed and students were randomly sampled within schools.

1.  For each cluster $i=1,\ldots,g$, draw a random sample of the rows for cluster $i$, with
    replacement, to form the bootstrap sample
    $\left(\boldsymbol{y}^*_i, \boldsymbol{X}^*_i, \boldsymbol{Z}^*_i \right)$.
2.  Refit the model to the bootstrap sample and extract the parameter
    estimates of interest.
3.  Repeat steps 1--2 $B$ times.

\noindent A third version of the cases bootstrap resamples both clusters and cases
within clusters. This is what @Field:2007vm term the two-state bootstrap. 
We would choose this resampling scheme if both schools and 
students were sampled during the data collection process. 

\noindent All three versions of the case bootstrap are implemented in \pkg{lmeresampler}. We explain how the resampling is specified in our [Overview of lmeresampler]. Regardless of which version of the cases bootstrap you choose, it requires the weakest conditions: it only requires that the hierarchical structure in the data set is correctly specified.

### The parametric bootstrap

The parametric bootstrap simulates random effects and error terms from the fitted 
distributions to form bootstrap resamples. Consequently,  it requires the strongest conditions---that is, the parametric bootstrap assumes that the model, as specified by Equations \@ref(eq:lme) and \@ref(eq:dsns), is correctly specified.

Let $\widehat{\boldsymbol \beta}$, $\widehat{\boldsymbol D}$, and $\widehat{\sigma}^2$ be maximum likelihood or restricted maximum likelihood estimates of the fixed effects and variance components from the fitted model. The parametric bootstrap is then implemented through the following steps:

1. Simulate $g$ error term vectors, $\boldsymbol{e}_i^*$, of length $n_i$ from $\mathcal{N}\left(\boldsymbol{0},\widehat{\sigma}^2 \boldsymbol{I}_{n_i} \right)$.
2. Simulate $g$ random effects vectors, $\boldsymbol{b}_i^*$, from $\mathcal{N}\left(\boldsymbol{0},\widehat{\boldsymbol{D}} \right)$.
3. Generate bootstrap responses $\boldsymbol{y}^*_i = \boldsymbol{X}_i \widehat{\boldsymbol{\beta}} + \boldsymbol{Z}_i \boldsymbol{b}_i^* + \boldsymbol{e}_i^*$.
4. Refit the model to the bootstrap responses and extract the parameter estimates of interest.
5. Repeat steps 2--4 $B$ times.


### The residual bootstrap

The residual bootstrap resamples the residual quantities from the fitted
LME model in order to generate bootstrap resamples. 
There are three general types of residuals for LME models [@Haslett2007-kl; @Singer2017-sd]. \dfn{Marginal residuals} correspond to the errors made using the marginal predictions, $\widehat{\boldsymbol r}_i = \boldsymbol{y_i} - \widehat{\boldsymbol{y}}_i = \boldsymbol{y}_i - \boldsymbol{X}_i \widehat{\boldsymbol{\beta}}$. \dfn{Conditional residuals} correspond to the errors made using predictions conditioned on the clusters, $\widehat{\boldsymbol{e}}_i = \boldsymbol{y}_i - \widehat{\boldsymbol{y}}_i| \boldsymbol{b}_i  = \boldsymbol{y}_i - \boldsymbol{X}_i \widehat{\boldsymbol{\beta}} - \boldsymbol{Z}_i \widehat{\boldsymbol{b}}_i$. The \dfn{predicted random effects} are the last type of residual quantity and are defined as $\widehat{\boldsymbol{b}}_i = \widehat{\boldsymbol{D}} \boldsymbol{Z}_i^\prime \widehat{\boldsymbol{V}}_i^{-1}\left(\boldsymbol{y}_i -  \boldsymbol{X}_i \widehat{\boldsymbol{\beta}} \right)$ where $\widehat{\boldsymbol{V}}_i = \boldsymbol{Z}_i \widehat{\boldsymbol{D}}\boldsymbol{Z}_i^\prime + \widehat{\sigma}^2 \boldsymbol{I}_{n_i}$.

A naive implementation of the residual bootstrap would draw random samples, with
replacement, from the estimated conditional residuals
and the best linear unbiased predictors (BLUPS);
however, this will consistently underestimate the variability in the
data because the residuals are shrunken toward zero
[@Morris:2002tj; @Carpenter:2003uy]. @Carpenter:2003uy solve
this problem by "reflating" the residual quantities so that the empirical 
covariance matrices match the estimated covariance matrices prior to resampling:

1.  Fit the model and calculate the empirical BLUPs, $\widehat{\boldsymbol{b}}_i$, and the
    predicted conditional residuals, $\widehat{\boldsymbol{e}}_i$.

2.  Mean center each residual quantity and reflate the centered
    residuals. Only the process to reflate the predicted random effects is discussed below, but the process is analogous for the conditional residuals.

    i. Arrange the random effects into a $g \times q$ matrix, where each row contains the predicted random effects from a single group. Denote this matrix as $\widehat{\boldsymbol{U}}$. Define $\boldsymbol{\widehat{\Gamma}} = \boldsymbol{I}_g  \otimes \widehat{\boldsymbol{D}}$, the block diagonal covariance matrix of $\widehat{\boldsymbol{U}}$.
    ii. Calculate the empirical covariance matrix as $\boldsymbol{S} = \widehat{\boldsymbol{U}}^\prime \widehat{\boldsymbol{U}} / g$. We follow the approach of @Carpenter:2003uy, dividing by the number of groups, $g$, rather than $g-1$.
    iii. Find a transformation of $\widehat{\boldsymbol{U}}$, $\widehat{\boldsymbol{U}}^* = \widehat{\boldsymbol{U}} \boldsymbol{A}$, such that $\widehat{\boldsymbol{U}}^{*\prime} \widehat{\boldsymbol{U}}^* / g = \boldsymbol{\widehat{\Gamma}}$. Specifically, we will find $\boldsymbol{A}$ such that $\boldsymbol{A}^\prime \widehat{\boldsymbol{U}}^\prime \widehat{\boldsymbol{U}} \boldsymbol{A} / g = \boldsymbol{A}^\prime \boldsymbol{SA} = \boldsymbol{\widehat{\Gamma}}$. The choice of $\boldsymbol{A}$ is not unique, so we use the recommendation given by @Carpenter:2003uy: $\boldsymbol{A} = \left(\boldsymbol{L}_D \boldsymbol{L}_S^{-1} \right)^\prime$ where $\boldsymbol{L}_D$ and $\boldsymbol{L}_S$ are the Cholesky factors of $\boldsymbol{\widehat{\Gamma}}$ and $\boldsymbol{S}$, respectively.

3. Draw a random sample, with replacement, from the set $\lbrace \boldsymbol{u}_i^* \rbrace$ of size $g$, where $\boldsymbol{u}_i^*$ is the $i$th row of the centered and reflated random effects matrix, $\widehat{\boldsymbol{U}}^*$. 

4. Draw $g$ random samples, with replacement, of sizes $n_i$ from the set of the centered and reflated conditional residuals, $\lbrace \boldsymbol{e}_i^* \rbrace$.

5. Generate the bootstrap responses, $\boldsymbol{y}^*_i$, using the fitted model equation: $\boldsymbol{y}^*_i = \boldsymbol{X}_i \widehat{\boldsymbol{\beta}} + \boldsymbol{Z}_i \widehat{\boldsymbol{u}}^*_i + \boldsymbol{e}_i^*$.

6.  Refit the model to the bootstrap responses and extract the parameter
    estimates of interest.

7. Repeat steps 3--6 B times.


Notice that the residual bootstrap is a _semiparametric_ bootstrap, since it depends on the model structure (both the mean function and the covariance structure) but not the distributional conditions [@Morris:2002tj]. 



### The random-effects block bootstrap

Another semiparametric bootstrap is the random effect block (REB) bootstrap proposed by @Chambers:2013ba. The REB bootstrap can be viewed as a version of the residual bootstrap where conditional residuals are resampled from within clusters (i.e., blocks) to allow for weaker assumptions on the covariance structure of the residuals. The residual bootstrap requires that the conditional residuals are independent and identically distributed, whereas the REB bootstrap relaxes this to only require that the covariance structure of the error terms is similar across clusters. In addition, the REB bootstrap utilizes the marginal residuals to calculate non-parametric predicted random effects rather than relying on the model-based empirical best linear unbiased predictors (EBLUPS). @Chambers:2013ba developed three versions of the REB bootstrap, all of which have been implemented in \pkg{lmeresampler}. We refer the reader to @Chambers:2013ba for a discussion of when each should be used. It's important to note that at the time of this writing, that the REB bootstrap has only been explored and implemented for use with two-level models.


#### REB/0

The base algorithm for the REB bootstrap (also known as REB/0) is as follows:

1. Calculate non-parametric residual quantities for the model.
    a. Calculate the marginal residuals for each group, $\widehat{\boldsymbol{r}}_i = \boldsymbol{y}_i - \boldsymbol{X}_i \widehat{\boldsymbol{\beta}}$.
    b. Calculate the non-parametric predicted random effects, $\tilde{\boldsymbol{b}}_i = \left( \boldsymbol{Z}_i^\prime \boldsymbol{Z}_i \right)^{-1} \boldsymbol{Z}^\prime_i \boldsymbol{r}_i$.
    c. Calculate the non-parametric conditional residuals using the residuals quantities obtained in the previous two steps, $\boldsymbol{\tilde{e}}_i = \widehat{\boldsymbol{r}}_i - \boldsymbol{Z}_i \boldsymbol{\tilde{b}}_i$.
2. Take a random sample, with replacement, of size $g$ from the set $\lbrace \tilde{\boldsymbol{b}}_i \rbrace$. Denote these resampled random effects as $\boldsymbol{\tilde{b}}^*_i$.
3. Take a random sample, with replacement, of size $g$ from the cluster ids. For each sampled cluster, draw a random sample, with replacement, of size $n_i$ from that cluster's vector of error terms, $\boldsymbol{\tilde{e}}_i$.
4. Generate bootstrap responses, $\boldsymbol{y}^*_i$, using the fitted model equation: $\boldsymbol{y}^*_i = \boldsymbol{X}_i \widehat{\beta} + \boldsymbol{Z}_i \boldsymbol{\tilde{b}}^*_i + \boldsymbol{\tilde{e}}^*_i$.
5. Refit the model to the bootstrap sample and extract the parameter estimates of interest.
6. Repeat steps 2--5 $B$ times.


#### REB/1

The first variation of the REB bootstrap (REB/1) zero centers and reflates the residual quantities prior to resampling in order to satisfy the conditions for consistency [@shao1995jackknife]. This is the same process outlined in Step 2 of the residual bootstrap outlined above. 


#### REB/2

The second variation of the REB bootstrap (REB/2 or postscaled REB) addresses two issues: potential non-zero covariances in the joint bootstrap distribution of the variance components and potential bias in the parameter estimates. After the REB/0 algorithm is run, the following post processing is performed:

##### Uncorrelate the variance components.

To uncorrelate the bootstrap estimates of the variance components produced by REB/0, @Chambers:2013ba propose the following procedure:

1. Apply natural logarithms to the bootstrap distribution of each variance component and form the following matrices. Note that we use $\nu$ to denote the total number of variance components.

    - $\boldsymbol{S}^*$: a $B \times \nu$ matrix formed by binding the columns of these distributions together
    - $\boldsymbol{M}^*$: a $B \times \nu$ matrix where each column contains the column mean from the corresponding column in $\boldsymbol{S}^*$
    - $\boldsymbol{D}^*$: a $B \times \nu$ matrix where each column contains the column standard deviation from the corresponding column in $\boldsymbol{S}^*$

2. Calculate $\boldsymbol{C}^*=\mathrm{cov} \left( \boldsymbol{S}^* \right)$.
3. Calculate $\boldsymbol{L}^* = \boldsymbol{M}^* + \lbrace \left(\boldsymbol{S}^* - \boldsymbol{M}^*\right)\boldsymbol{C}^{*-1/2} \rbrace \circ \boldsymbol{D}^*$, where $\circ$ denotes the Hadamard (elementwise) product.
4. Exponentiate the elements of $\boldsymbol{L}^*$. The columns of $\boldsymbol{L}^*$ are then uncorrelated versions of the bootstrap variance components.

##### Center the bootstrap estimates at the original estimate values.

To correct bias in the estimation of the fixed effects, apply a mean correction. For each parameter estimate, $\widehat{\beta}_k$, adjust the bootstrapped estimates, $\widehat{\boldsymbol{\beta}}_k^*$ as follows: $\widetilde{\boldsymbol{\beta}}_k^{*} = \widehat{\beta}_k \boldsymbol{1}_{B} + \widehat{\boldsymbol{\beta}}_k^* -{\rm avg} \left( \widehat{ \boldsymbol{\beta} }_k^* \right)$.
To correct bias in the estimation of the variance components, apply a ratio correction. For each estimated variance component , $\widehat{\sigma}^2_v$, adjust the uncorrelated bootstrapped estimates, $\widehat{\boldsymbol{\sigma}}_v^{2*}$ as follows: $\widetilde{\boldsymbol{\sigma}}_{v}^{2*} = \widehat{\boldsymbol{\sigma}}_v^{2*} \circ \lbrace \widehat{\sigma}^2_v / {\rm avg} \left( \widehat{\boldsymbol{\sigma}}_v^{2*} \right) \rbrace$



### The wild bootstrap

The wild bootstrap also relaxes the assumptions made on the error terms of the model, allowing heteroscedasticity both within and across groups. The wild bootstrap is well developed for the ordinary regression model [@Liu1988-zw; @Flachaire2005-qo; @Davidson2008-vq] and @Modugno2015-kd adapt it for the nested LME model. 

To begin, we can re-express model \@ref(eq:lme) as

\begin{equation}
\boldsymbol{y}_i = \boldsymbol{X}_i \boldsymbol{\beta} + \boldsymbol{v}_i, \text{ where } \boldsymbol{v}_i = \boldsymbol{Z}_i \boldsymbol{b}_i + \boldsymbol{\varepsilon}_i. 
(\#eq:wild)
\end{equation}

\noindent The wild bootstrap proceeds as follows:

1. Draw a random sample, $w_1, w_2, \ldots, w_g$, from an auxiliary distribution with mean zero and unit variance.

2. Generate bootstrap responses using the re-expressed model equation \@ref(eq:wild): $\boldsymbol{y}^*_i = \boldsymbol{X}_i \widehat{\boldsymbol{\beta}} + \tilde{\boldsymbol{v}}_i w_j$, where $\tilde{\boldsymbol{v}}_i$ is a heteroscedasticity consistent covariance matrix estimator. @Modugno2015-kd suggest using what @Flachaire2005-qo calls ${{\rm HC}_2}$ or ${{\rm HC}_3}$ in the regression context:

    \begin{align}
{{\rm HC}_2}&: \tilde{\boldsymbol{v}}_i = {\rm diag} \left( \boldsymbol{I}_{n_i} - \boldsymbol{H}_i \right)^{-1/2} \circ \boldsymbol{r}_i\\
{{\rm HC}_3}&: \tilde{\boldsymbol{v}}_i = {\rm diag} \left( \boldsymbol{I}_{n_i} - \boldsymbol{H}_i \right) \circ \boldsymbol{r}_i,
\end{align}

    where  $\boldsymbol{H}_i = \boldsymbol{X}_i \left(\boldsymbol{X}_i^\prime \boldsymbol{X}_i \right)^\prime \boldsymbol{X}_i^\prime$, the $i$th diagonal block of the orthogonal projection matrix, and  $\boldsymbol{r}_i$ is the vector of marginal residuals for group $i$.

3. Refit the model to the bootstrap sample and extract the parameter estimates of interest.

4. Repeat steps 1--3 $B$ times.


Five options for the  auxiliary distribution are implemented in \pkg{lmeresampler}:

- The two-point distribution proposed by @Mammen1993-eo takes value $w_i=-(\sqrt{5} - 1)/2$ with probability $p=(\sqrt{5}+1) / (2 \sqrt{5})$ and $w_i = (\sqrt{5} + 1)/2$ with probability $1-p$.

- The two-point Rademacher distribution places equal probability on $w_i = \pm 1$.

- The six-point distribution proposed by @Webb2013-nw places equal probability on $w_i = \pm \sqrt{1/2}, \pm1, \pm \sqrt{3/2}$.

- The standard normal distribution.

- The gamma distribution with shape parameter 4 and scale parameter $1/2$ [@Liu1988-zw] that is centered to have mean zero. $w_i^*$ are randomly sampled from the gamma distribution, and then centered via $w_i = w_i^* - 2$.


@Modugno2015-kd found the Mammen distribution to be preferred based on a Monte Carlo study, but only compared it to the Rademacher distribution.

## Overview of lmeresampler


The \pkg{lmeresampler} package implements the five bootstrap procedures outlined in the previous section for Gaussian response
LME models for clustered data structures fit using either \CRANpkg{nlme} [@nlme] or \pkg{lme4}. The workhorse function in \pkg{lmeresampler} is 

```{r eval=FALSE}
bootstrap(model, .f, type, B, resample = NULL, reb_type = NULL, hccme, aux.dist,
          orig_data, .refit)
```

\noindent The four required parameters to \code{bootstrap()} are:

* `model`, an `lme` or `merMod` fitted model object.

* `.f`, a function defining the parameter(s) of interest that should be extracted/calculated for each bootstrap iteration.

* `type`, a character string specifying the type of bootstrap to run. Possible values include: `"parametric"`, `"residual"`, `"reb"`, `"wild"`, and `"case"`.

* `B`, the number of bootstrap resamples to generate.

* `.refit`, whether the model should be refit to the bootstrap sample. This defaults to `TRUE`.

There are also five optional parameters: `resample`, `reb_type`, `hccme`, `aux.dist`, and `orig_data`. 

* If the user sets `type = "case"`, then they must also set `resample` to specify what cluster resampling scheme to use. `resample` requires a logical vector of length equal to the number of levels in the model. A value of `TRUE` in the $i$th position indicates that cases/clusters at that level should be resampled. For example, to only resample the clusters (i.e., level 2 units) in a two-level model, the user would specify `resample = c(FALSE, TRUE)`. 

* If the user sets `type = "reb"`, then they must also set `reb_type` to indicate which version of the REB bootstrap to run. `reb_type` accepts the integers `0`, `1`, and `2` to indicate REB/0, REB/1, and REB/2, respectively.

* If the user sets `type = "wild"`, then they must specify both `hccme` and `aux.dist`. Currently, `hccme` can be set to `"hc2"` or `"hc3"` and `aux.dist` can be set to `"mammen"`, `"rademacher"`, `"norm"`, `"webb"`, or `"gamma"`.

* If variables are transformed within the model formula and `lmer()` is used to fit the LME model, then `orig_data` should be set to the original data frame, since this cannot be recovered from the fitted model object.


The `bootstrap()` function is a generic function that calls functions for each type of bootstrap. The user can call the specific bootstrap function directly if preferred. An overview of the specific bootstrap functions is given in Table \@ref(tab:boots).

```{r boots, eval = knitr::is_html_output(), echo=FALSE}
tibble::tribble(
  ~Bootstrap,  ~`Function name`, ~`Required arguments`,
  "Cases",       "`case_bootstrap`",        "`model, .f, type, B, resample, orig_data, .refit`", 
    "Residual",    "`resid_bootstrap`",    "`model, .f, type, B, orig_data, .refit`", 
    "REB",         "`reb_bootstrap`",        "`model, .f, type, B, reb_type, orig_data, .refit`",
    "Wild",        "`wild_bootstrap`",        "`model, .f, type, B, hccme, aux.dist, orig_data, .refit`",
    "Parametric",  "`parametric_bootstrap`",  "`model, .f, type, B, orig_data, .refit`"  
) |>
  knitr::kable(format = "html", caption = "Summary of the specific bootstrap functions called by `bootstrap()` and their required arguments.")
```


\begin{table}
\centering
\begin{tabular}{l l l} \toprule
Bootstrap  & Function name         & Required arguments\\ \midrule
Cases      & \code{case\_bootstrap}       & \code{model, .f, type, B, resample, orig\_data, .refit} \\
Residual   & \code{resid\_bootstrap}   & \code{model, .f, type, B, orig\_data, .refit} \\
REB        & \code{reb\_bootstrap}       & \code{model, .f, type, B, reb\_type, orig\_data, .refit}\\
Wild       & \code{wild\_bootstrap}       & \code{model, .f, type, B, hccme, aux.dist, orig\_data, .refit} \\
Parametric & \code{parametric\_bootstrap} & \code{model, .f, type, B, orig\_data, .refit} \\ \bottomrule
\end{tabular}
\caption{Summary of the specific bootstrap functions called by \samp{bootstrap()} and their required arguments.}
\label{tab:boots}
\end{table}



Each of the specific bootstrap functions performs four general steps:

1. *Setup.* Key information (parameter estimates, design matrices, etc.) is extracted from the fitted model to eliminate repetitive actions during the resampling process.

2. *Resampling.* The setup information is passed to an internal resampler function to generate the `B` bootstrap samples.

3. *Refitting.* The model is refit for each of the bootstrap samples and the specified parameters are extracted/calculated.

4. *Clean up.* An internal completion function formats the original and bootstrapped quantities to return a list to the user.

\noindent Each function returns an object of class `lmeresamp`, which is a list with elements outlined in Table \@ref(tab:return). `print()`, `summary()`, `plot()`, and `confint()` methods are available for `lmeresamp` objects.

```{r return, eval = knitr::is_html_output(), echo=FALSE}
tibble::tribble(
  ~Element,   ~Description,
  "`observed`", "values for the original model parameter estimates.",
  "`model`",       "the original fitted model object." ,
  "`.f`",          "the function call defining the parameters of interest.",
  "`replicates`",  "a $B \\times p$ tibble containing the bootstrapped quantities. Each column contains a single bootstrap distribution.",
  "`stats`",        "a tibble containing the `observed`, `rep.mean` (bootstrap mean), `se` (bootstrap standard error), and `bias` values for each parameter.",
  "`B`",           "the number of bootstrap resamples performed.",
  "`data`",        "the original data set.",
  "`seed`",        "a vector of randomly generated seeds that are used by the bootstrap.",
  "`type`",        "a character string specifying the type of bootstrap performed.",
  "`call`",        "the user's call to the bootstrap function.",
"`message`",     "a list of length `B` giving any messages generated during refitting. An entry will be `NULL` if no message was generated.", 
"`warning`",     "a list of length `B` giving any warnings generated during refitting. An entry will be `NULL` if no warning was generated.", 
"`error`",       "a list of length `B` giving any errors encountered during refitting. An entry will be `NULL` if no error was encountered."
) |>
  knitr::kable(format = "html", caption = "Summary of the specific bootstrap functions called by `bootstrap()` and their required arguments.")
```

\begin{table}[t]
\centering
\begin{tabular}{l p{5in}} \toprule
Element  & Description\\ \midrule
\code{observed}   & values for the original model parameter estimates.  \\ 
\code{model}      & the original fitted model object.   \\
\code{.f}         & the function call defining the parameters of interest.   \\
\code{replicates} & a $B \times p$ tibble containing the bootstrapped quantities. Each column contains a single bootstrap distribution.   \\
\code{stats}      &  a tibble containing the \code{observed}, \code{rep.mean} (bootstrap mean), \code{se} (bootstrap standard error), and \code{bias} values for each parameter.\\
\code{B}          & the number of bootstrap resamples performed.\\
\code{data}       & the original data set.\\
&\\
\code{seed}       &  a vector of randomly generated seeds that are used by the bootstrap.\\
\code{type}       & a character string specifying the type of bootstrap performed.\\
\code{call}       & the user's call to the bootstrap function. \\ 
\code{message}    & a list of length \code{B} giving any messages generated during refitting. An entry will be \code{NULL} if no message was generated. \\ 
\code{warning}    & a list of length \code{B} giving any warnings generated during refitting. An entry will be \code{NULL} if no warning was generated.\\ 
\code{error}      & a list of length \code{B} giving any errors encountered during refitting. An entry will be \code{NULL} if no error was encountered.\\ \bottomrule
\end{tabular}
\caption{Summary of the values returned by the bootstrap functions.}
\label{tab:return}
\end{table}


\pkg{lmeresampler} also provides the `extract_parameters()` helper function to extract the fixed effects and variance components from `merMod` and `lme` objects as a named vector. 


## Example applications

### A two-level example: JSP data

As a first application of the bootstrap for nested LME models, consider the junior school project (JSP) data [@goldstein2011; @Mortimore1988-xv] that is stored as `jsp728` in \pkg{lmeresampler}. The data set is comprised of measurements taken on 728 elementary school students across 48 schools in London.

```{r}
library(lmeresampler)
tibble::as_tibble(jsp728)
```

Suppose we wish to fit a model using the math score at age 8, gender, and the father's social class to describe math scores at age 11, including a random intercept for school [@goldstein2011]. This LME model can be fit using the `lmer()` function in \pkg{lme4}.

```{r message=FALSE}
library(lme4)
jsp_mod <- lmer(mathAge11 ~ mathAge8 + gender + class + (1 | school), data = jsp728)
```

To implement the residual bootstrap to estimate the fixed effects, we can use the `bootstrap()` function and set `type = "residual"`.

```{r cache=TRUE}
(jsp_boot <- bootstrap(jsp_mod, .f = fixef, type = "residual", B = 2000))
```

\noindent We can then calculate normal, percentile, and basic bootstrap confidence intervals via `confint()`.

```{r}
confint(jsp_boot)
```

The default setting is to calculate all three intervals, but this can be restricted by setting the `type` parameter to `"norm"`, `"basic"`, or `"perc"`.


### User-specified statistics: Estimating repeatability/intraclass correlation

The beauty of the bootstrap is its flexibility. Interval estimates can be constructed for functions of model parameters that would otherwise require more complex derivations. For example, the bootstrap can be used to estimate the intraclass correlation. The intraclass correlation measures the proportion of the total variance in the response accounted for by groups, and is an important measure of repeatability in ecology and evolutionary biology [@Nakagawa2010-co]. As a simple example, we'll consider the `BeetlesBody` data set in \pkg{rptR} [@rptR]. This simulated data set contains information on body length (`BodyL`) and the `Population` from which the beetles were sampled. A simple Guassian-response LME model of the form

$$
y_{ij} = \beta_0 + b_i + \varepsilon_{ij}, \qquad b_i \sim \mathcal{N}(0, \sigma^2_b), \qquad \varepsilon_{ij} \sim \mathcal{N}(0, \sigma^2),
$$

\noindent can be used to describe the body length of beetle $j$ from population $i$. The repeatability is then calculated as $R = \sigma^2_b / (\sigma^2_b + \sigma^2)$. Below we fit this model using `lmer()`:

```{r}
data("BeetlesBody", package = "rptR")
(beetle_mod <- lmer(BodyL ~ (1 | Population), data = BeetlesBody))
```

To construct a bootstrap confidence interval for the repeatability, we first must write a function to calculate it from the fitted model. Below we write a one-off function for this model to demonstrate a "typical" workflow rather than trying to be overly general.

```{r}
repeatability <- function(object) {
  vc <- as.data.frame(VarCorr(object))
  vc$vcov[1] / (sum(vc$vcov))
}
```

\noindent The original estimate of repeatability can then be quickly calculated:

```{r}
repeatability(beetle_mod)
```

\noindent To construct a bootstrap confidence interval for the repeatability, we run the desired bootstrap procedure, specifying `.f = repeatability` and then pass the results to `confint()`.

```{r cache=TRUE}
(beetle_boot <- bootstrap(beetle_mod, .f = repeatability, type = "parametric", B = 2000))
(beetle_ci <- confint(beetle_boot, type = "basic"))
```

Notice that the \code{term} column of `beetle_ci` is an empty character string since we did not have \code{repeatability} return a named vector.

Alternatively, we can `plot()` the results, as shown in Figure \@ref(fig:density). The plot method for `lmeresamp` objects uses `stat_halfeye()` from \CRANpkg{ggdist} [@ggdist] to render a density plot with associated 66% and 95% percentile intervals.

```{r density, fig.cap="Density plot of the repeatabilities from the beetle model. The median bootstrap repeatability is approximatley .28 and denoted by a point under the density.  66\\% and 95\\% confidence intervals for the bootstrap repeatability are (0.217, 0.394) and (0.118, 0.475), respectively, and are displayed as line segments below the density.", fig.alt = "The figure is a density plot displaying the bootstrap distribution of the repeatability. Repeatability is displayed on the x-axis and density is displayed on the y-axis. A point marks the median repeatability on the x-axis, with the 66% and 95% bootstrap percentile intervals marked using lines.", fig.height = 2.25, fig.width = 3, fig.align='center', out.width = "40%"}
plot(beetle_boot, .width = c(.5, .9)) + 
  ggplot2::labs(
    title = "Bootstrap repeatabilities",
    y = "density",
    x = "repeatability"
  )
```


### Bootstrap tests for a single parameter

While \pkg{lmeresampler} was designed with a focus on estimation, the
bootstrap functions can be used to conduct bootstrap tests on individual parameters. For example, returning to the JSP example, we might be interested in generating approximate $p$-values for the fixed effects:

```{r}
summary(jsp_mod)$coefficients
```

To generate a bootstrap $p$-value for a fixed effect, we must generate $B$ bootstrap resamples under the reduced model (i.e., the null hypothesis), refit the full model, and then calculate the $t$-statistic for each resample, denoted $t^*_i$. The bootstrap $p$-value is then calculated as $(n_{\rm extreme} + 1) / (B + 1)$ [@davison-hinkley; @pbkrtest].

Using the `bootstrap()` function, you can implement this procedure for a single parameter. For example, if we wish to calculate the bootstrap $p$-value for the `class` fixed effect we first generate $B=1000$ bootstrap samples from the reduced model without `class`. In this example, we use the Wild bootstrap to illustrate that we are not restricted to a parametric bootstrap.

```{r cache=TRUE}
reduced_model <- update(jsp_mod, . ~ . - class)
reduced_boot <- bootstrap(reduced_model, type = "wild", B = 1000, hccme = "hc2", 
                          aux.dist = "mammen", .refit = FALSE)
```

\noindent Next, we refit the full model, `jsp_mod`, to each simulation and extract the $t$-statistic for the `class` variable. Note that the function `extract_t()` extracts the specified $t$-statistic from the coefficient table from model summary.

```{r cache=TRUE}
extract_t <- function(model, term) {
  coef(summary(model))[term, "t value"]
}

tstats <- purrr::map_dbl(
  reduced_boot, 
  ~refit(jsp_mod, .x) %>% extract_t(., term = "classnonmanual")
)
```

\noindent With the bootstrap $t$-statistics in hand, we can approximate the $p$-value using basic logical and arithmetic operators

```{r}
(sum(abs(tstats) >= extract_t(jsp_mod)) + 1) / (1000 + 1)
```

While the above process is not particularly difficult to implement using the tools provided by \pkg{lmeresampler}, things get tedious if multiple parameters are of interest in this summary table. To help reduce this burden on the user, we have provided the `bootstrap_pvals()` function that will add bootstrap $p$-values for each term in the coefficient summary table. For `jsp_mod`, this is achieved in the below code chunk:

```{r cache=TRUE}
bootstrap_pvals(jsp_mod, type = "wild", B = 1000, hccme = "hc2", aux.dist = "mammen")
```

It's important to note that running bootstraps for each term in the model is computationally demanding. To speed up the computation, you can run the command in parallel, as we discuss below in [Bootstrapping in parallel].


### Model comparison

The bootstrap can be useful during model selection.
For example, if you are comparing a full and reduced model where the reduced model has fewer random effects, a 50:50 mixture of $\chi^2$ distributions is often used [@Stram:1994wd]; however, @Pinhiero:2000vf point out that this approximation is not always optimal. In this example, we explore the `Machine` data set discussed by @Pinhiero:2000vf, which consists of productivity scores for six workers on three brands of machine. This data set can be loaded from \pkg{nlme}:

```{r}
data("Machines", package = "nlme")
```

@Pinhiero:2000vf consider two LME models for these data. The first model has a fixed effect for the machine and a random intercept for the worker.

```{r}
reduced_mod <- lmer(score ~ Machine + (1 | Worker), data = Machines, REML = FALSE)
```

The second model has the same fixed effects structure, but adds an additional random effect for the machine within the worker.

```{r}
full_mod <- lmer(score ~ Machine + (1 | Worker/Machine), data = Machines, REML = FALSE)
```

@Pinhiero:2000vf note that the approximate null distribution given by $0.5\chi^2_0 + 0.5 \chi^2_1$ is not successful when the models are fit via maximum likelihood, and that the mixture is closer to $0.65\chi^2_0 + 0.35 \chi^2_1$. Instead of relying on the conventional approximation, a bootstrap test can be conducted using `bootstrap()` to simulate the responses from the reduced model.

To conduct this bootstrap test, we first extract the observed statistic obtained via `anova()` and then generate $B=1000$ bootstrap responses from the reduced model, `fm1_machine`. Recall that specifying `.refit = FALSE` returns a data frame of the simulated responses. Here, we use a residual bootstrap for illustration.

```{r cache=TRUE}
observed <- anova(full_mod, reduced_mod)$Chisq[2]

reduced_boot <- bootstrap(reduced_mod, type = "residual", B = 1000, .refit = FALSE)
```

Next, we must fit both the full and reduced models to the bootstrap responses and calculate the test statistic. The user-written `compare_models()` function performs this refit and calculation for given models and bootstrap responses. The `control` argument for the full model was set to reduce the number of convergence warnings, since the null model had a variance component of 0 for machines within workers, so we expect warnings as we fit an expanded model.

```{r cache=TRUE}
compare_models <- function(full, reduced, newdata) {
  full_mod <- refit(full, newdata, 
                    control = lmerControl(check.conv.singular = "ignore", 
                                          check.conv.grad = "ignore"))
  reduced_mod <- refit(reduced, newdata)
  anova(full_mod, reduced_mod)$Chisq[2]
}
 
chisq_stats <- purrr::map_dbl(reduced_boot, ~compare_models(full_mod, reduced_mod, newdata = .x))
```

With the test statistics in hand, we can quickly calculate the $p$-value

```{r}
(sum(chisq_stats >= observed) + 1) / (1000 + 1)
```



### Simulation-based model diagnostics

Our final example illustrates how the `bootstrap()` function can be used for model diagnosis. @Loy2017-fo propose using the lineup protocol to diagnose LME models, since artificial structures often appear in conventional residual plots for this model class that are not indicative of a model deficiency. 

In this example, we consider the `Dialyzer` data set provided by \pkg{nlme}. The data arise from a study characterizing the water transportation characteristics of 20 high flux membrane dialyzers, which were introduced to reduce the time a patient spends on hemodialysis [@Vonesh:1992us]. The dialyzers were studied in vitro using bovine blood at flow rates of either 200 or 300 ml/min. The study measured the the ultrafiltration rate (ml/hr) at even transmembrane pressures (in mmHg). @Pinhiero:2000vf discuss modeling these data. Here, we explore how to create a lineup of residual plots to investigate the adequacy of the initial homoscedastic LME model fit by @Pinhiero:2000vf. 

```{r}
library(nlme)
dialyzer_mod <- lme(
  rate ~ (pressure + I(pressure^2) + I(pressure^3) + I(pressure^4)) * QB, 
  data = Dialyzer, 
  random = ~ pressure + I(pressure^2)
)
```

@Pinhiero:2000vf construct a residual plot of the conditional residuals plotted against the transmembrane pressure to explore the adequacy of the fitted model (Figure \@ref(fig:residdialyzer)). There appears to be increasing spread of the conditional residuals, which would indicate that the homoscedastic model is not sufficient.

```{r residdialyzer, echo = FALSE, fig.cap="A plot of the conditional residuals against the transmembrane pressure for the dialyzer model. It appears that the variability of the conditional residuals increases with transmembrance pressure, but does this indicate a model condition is violated?", fig.alt = "A plot of the residuals on the y-axis and the transmembrane pressure on the x-axis. A line is draw at y = 0 and the spread of the points around this line is increasing with transmembrane pressure.", fig.height = 2.5, fig.width = 3, fig.align='center', out.width="40%"}
library(ggplot2)
ggplot(Dialyzer, aes(x = pressure, y = resid(dialyzer_mod))) +
  geom_hline(yintercept = 0, color = "gray60") +
  geom_point(shape = 1) +
  theme_bw() +
  labs(x = "Transmembrane pressure (dmHg)", y = "Residuals (ml/hr)")
```

To check if this pattern is actually indicative of a problem, we construct a lineup of residual plots. To do this, we must generate data from a number of properly specified models, say 19, to serve as decoy residual plots. Then, we create a faceted set of residual plots where the observed residual plot (Figure \@ref(fig:residdialyzer)) is randomly assigned to a facet. To generate the residuals from properly specified models, we use the parametric bootstrap via `bootstrap()` and extract a data frame containing the residuals from each bootstrap sample using `hlm_resid()`  from \CRANpkg{HLMdiag} [@hlmdiag].

```{r cache=TRUE}
set.seed(1234)
library(HLMdiag)
sim_resids <- bootstrap(dialyzer_mod, .f = hlm_resid, type = "parametric", B = 19)
```

\noindent The simulated residuals are stored in the `replicates` element of the `sim_resids` list. `sim_resids$replicates` is a tibble containing the 19 bootstrap samples, with the the replicate number stored in the `.n` column.

```{r}
dplyr::glimpse(sim_resids$replicates)
```

Now, we use the `lineup()` function from \CRANpkg{nullabor} [@buja2009] to generate the lineup data. `lineup()` will randomly insert the observed (`true`) data into the `samples` data, "encrypt" the position of the observed data, and print a message that you can later decrypt in the console. 

```{r}
library(nullabor)
lineup_data <- lineup(true = hlm_resid(dialyzer_mod), n = 19, samples = sim_resids$replicates)
dplyr::glimpse(lineup_data)
```

\noindent With the lineup data in hand, we can create a lineup of residual plots using `facet_wrap()`:

```{r lineup, fig.cap="A lineup of the conditional residuals against the transmembrane pressure for the dialyzer model. One of the facets contains the true residual plot generated from the fitted model, the others are decoys generated using a parametric bootstrap. Facet 13 contains the observed residuals and is discernibly different from decoy plots, providing evidence of that a model condition has been violated.", fig.alt = "A lineup plot comprised of 20 facets that all display plots of the conditional residuals on the y-axis and transmembrance pressure on the x-axis. The facets are numbered 1 to 20. Nineteen of the plots display simulated residuals, which show little structure. Facet 13 displays the observed data, which shows increasing spread of the residuals.", fig.height = 6.875, fig.height = 6, fig.align='center'}
ggplot(lineup_data, aes(x = pressure, y = .resid)) +
  geom_hline(yintercept = 0, color = "gray60") +
  geom_point(shape = 1) +
  facet_wrap(~.sample) +
  theme_bw() +
  labs(x = "Transmembrane pressure (dmHg)", y = "Residuals (ml/hr)")
```


\noindent In Figure \@ref(fig:lineup), the observed residual plot is in position `r attr(lineup_data, "pos")`. If you can discern this plot from the field of decoys, then there is evidence that the fitted homogeneous LME model is deficient. In this case,  we believe `r attr(lineup_data, "pos")` is discernibly different, as expected based on the discussion in @Pinhiero:2000vf. 

Since we have discovered signs of within-group heteroscedasticity, we could reformulate our model via `nlme::lme()` adding a `weights` argument using `varPower`, or we could utilize the Wild bootstrap, as illustrated below.

```{r cache=TRUE}
wild_dialyzer <- bootstrap(dialyzer_mod, .f = fixef, type = "wild", B = 1000, 
                           hccme = "hc2", aux.dist = "webb")

confint(wild_dialyzer, type = "perc")
```



<!-- We refer the reader to @Loy2017-fo for a discussion of additional diagnostic lineup plots. -->

<!-- Other simulation-based diagnostics have been proposed in the literature and can be implemented with the aid of \pkg{lmeresampler}. For example, @Longford:2001wy proposes..., which could be implemented by... -->







## Bootstrapping in parallel

```{r include=FALSE, eval=FALSE}
library(foreach)
library(doParallel)
nboot <- rep(1000, 2)

set.seed(5678)

# Starting a cluster with 2 cores
no_cores <- 2
cl <- makeCluster(no_cores)
registerDoParallel(cores = no_cores)

# Run 1000 bootstrap iterations on each core
ptime <- system.time({
  boot_parallel <- foreach(
    B = nboot, 
    .combine = combine_lmeresamp,
    .packages = c("lmeresampler", "lme4")
  ) %dopar% {
    bootstrap(jsp_mod, .f = fixef, type = "parametric", B = B)
  }
})

# Stop the cluster
stopCluster(cl)

stime <- system.time({
  boot_parallel <- foreach(
    B = nboot, 
    .combine = combine_lmeresamp,
    .packages = c("lmeresampler", "lme4")
  ) %do% {
    bootstrap(jsp_mod, .f = fixef, type = "parametric", B = B)
  }
})
```

Bootstrapping is a computationally demanding task, but bootstrap iterations do not rely on each other so they are easy to implement in parallel. Rather than building parallel processing into \pkg{lmeresampler}, we created a utility function, `combine_lmeresamp()`, that allows the user to easily implement parallel processing using \pkg{doParallel} [@doparallel] and \pkg{foreach} [@foreach]. The code is thus concise and simple enough for users without much experience with parallelization, while also providing flexibility to the user. 


\CRANpkg{doParallel} and \CRANpkg{foreach} default to multicore (i.e., forking) functionality when using parallel computing on UNIX operating systems and snow-like (i.e., clustering) functionality on Windows systems. In this section, we will use clustering in our example. For more information on forking, we refer the reader to the vignette in @doparallel.


The basic idea behind clustering is to execute tasks as a "cluster" of computers. Each cluster needs to be fed in information separately, and as a consequence clustering has more overhead than forking. Clusters also need to be made and stopped with each call to `foreach()` to explicitly tell the CPU when to begin and end the parallelization.

Below, we revisit the JSP example and distribute 2000 bootstrap iterations equally over two cores:

```{r cache=TRUE}
library(foreach)
library(doParallel)

set.seed(5678)

# Starting a cluster with 2 cores
no_cores <- 2
cl <- makeCluster(no_cores)
registerDoParallel(cores = no_cores)

# Run 1000 bootstrap iterations on each core
boot_parallel <- foreach(
  B = rep(1000, 2), 
  .combine = combine_lmeresamp,
  .packages = c("lmeresampler", "lme4")
) %dopar% {
  bootstrap(jsp_mod, .f = fixef, type = "parametric", B = B)
}

# Stop the cluster
stopCluster(cl)
```

\noindent The `combine_lmeresamp()` function combines the two `lmeresamp` objects that are returned from the two `bootstrap()` calls into a single `lmeresamp` object. Consequently, working with the returned object proceeds as previously discussed.

It's important to note that running a process on two cores does not yield a runtime that is twice as fast as running the same process on one core. This is because parallelization takes some overhead to split the processes, so while runtime will substantially improve, it will not correspond exactly to the number of cores being used. For example, the runtime for the JSP example run on a single core was

```{r echo=FALSE}
stime <- readr::read_rds("serial_time.RDS")
stime
```

\noindent and the runtime for the JSP run on two cores was

```{r echo=FALSE}
ptime <- readr::read_rds("parallel_time.RDS")
ptime
```

\noindent These timings were generated using `system.time()` on a MacBook Pro with a 2.9 GHz Quad-Core Intel Core i7 processor. In this set up, running the 2000 bootstrap iterations over two cores reduced the runtime by a factor of about `r round(stime[3]/ptime[3], 2)`, but this will vary based on the hardware and setting used.



## Summary

In this paper, we discussed our implementation of five bootstrap procedures for nested, Gaussian-response LME models fit via the \pkg{nlme} or \pkg{lme4} packages. The \code{bootstrap()} function in \pkg{lmeresampler} provides a unified interface to these procedures, allowing users to easily bootstrap their fitted LME models. In our examples, we illustrated the basic usage of the bootstrap, how it can be used to estimate functions of parameters, how it can be used for testing, and how it can be used to create simulation-based visual diagnostics. The bootstrap approach to inference is computationally intensive, so we have also demonstrated how users can bootstrap in parallel. 

While this paper focused solely on the nested, Gaussian-response LME model, \pkg{lmeresampler} implements bootstrap procedures for a wide class of models. Specifically, the cases, residual, and parametric bootstraps can be used to bootstrap generalized LME models fit via \code{lme4::glmer()}. Additionally, the parametric bootstrap works with LME models with crossed random effects, though the results may not be optimal [@Mccullagh2000-st]. Future development of \pkg{lmeresampler} will focus on implementing additional extensions, especially for crossed data structures.

## Acknowledgements

We thank Spenser Steele for his contributions to the original code base of \pkg{lmeresampler}. We also thank the reviewers and associate editor whose comments improved the quality of this paper.
