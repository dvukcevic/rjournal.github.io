---
title: Limitations in Detecting Multicollinearity due to Scaling Issues in the mcvis
  Package
abstract: |
  Transformation of the observed data is a very common practice when a troubling degree of near multicollinearity is detected in a linear regression model. However, it is important to take into account that these transformations may affect the detection of this problem, so they should not be performed systematically. In this paper we analyze the transformation of the data when applying the R package mcvis, showing that it only detects essential near multicollinearity when the *studentise* transformation is performed.
draft: no
author:
- name: Roman Salmeron Gomez
  affiliation: University of Granada
  address:
  - Department of Quantitative Methods for Economics and Busines
  - Poligono La Cartuja sn, 18071, Granada, Spain.
  url: http://metodoscuantitativos.ugr.es/pages/web/romansg
  orcid: 0000-0003-2589-4058
  email: romansg@ugr.es
- name: Catalina B. Garcia Garcia
  url: http://metodoscuantitativos.ugr.es/pages/web/cbgarcia
  email: cbgarcia@ugr.es
  orcid: 0000-0003-1622-3877
  affiliation: University of Granada
  address:
  - Department of Quantitative Methods for Economics and Busines
  - Poligono La Cartuja sn, 18071, Granada, Spain.
- name: Ainara Rodriguez Sanchez
  url: http://metodoscuantitativos.ugr.es/pages/web/cbgarcia
  email: arsanchez@cee.uned.es
  orcid: 0000-0003-1622-3877
  affiliation: U.N.E.D.
  address:
  - Department of Applied Economics and Economic History
  - Madrid, Spain.
- name: Claudia Garcia Garcia
  url: http://metodoscuantitativos.ugr.es/pages/web/cbgarcia
  email: clgarc13@ucm.es
  orcid: 0000-0003-1622-3877
  affiliation: Complutense University of Madrid
  address:
  - Department of Applied Economics, Structure and History
  - Madrid, Spain.
type: package
output:
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: salmeron-etal.bib
date: '2023-02-10'
date_received: '2021-12-06'
volume: 14
issue: 4
slug: RJ-2023-010
journal:
  lastpage: 279
  firstpage: 264

---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(rjtools)
library(ISLR)
library(mcvis)
library(plotly)
library(ggplot2)
library(multiColl)
library(tourr)
library(gifski)
```


# Introduction

Given the model $\mathbf{Y} = \mathbf{X} \cdot \boldsymbol{\beta} + \mathbf{u}$ for $n$ observations and $p$ independent variables where $\mathbf{Y}$ is a vector that contains the observations of the dependent variables, $\mathbf{X} = [\mathbf{1} \ \mathbf{X}_{2} \dots \mathbf{X}_{p}]$ is a matrix whose columns contain the observations of the independent variables (where the first column is a vector of ones representing the intercept) and $\mathbf{u}$ represents the spherical random disturbance, the existence of linear relationships between the independent variables of a model is known as multicollinearity. It is well-known that a high degree of multicollinearity can affect the analysis of a linear regression model. In this case, it is said that the multicollinearity is troubling [@Novales1988; @ramanathan; @Wooldridge2008; @Gujarati2010]. It is also interesting to note the distinction made, for example, by  @Marquardt1980 or @SneeMarquardt1984, between essential (near-linear relationship between at least two independent variables excluding the intercept) and non-essential multicollinearity (near-linear relationship between the intercept and at least one of the remaining independent variables).
 
Note that the detection process is key to determining which tool is best suited to mitigation of the problem: for example, ridge regression of @HoerlKennard1970a and @HoerlKennard1970a; LASSO of @Tibshirani1996 or elastic net of @ZouHastie, among others.

The most commonly applied measures to detect whether the degree of multicollinearity is troubling are the following:
 
* The Variation Inflation Factor (VIF) that is obtained with the following expression $VIF(j) = \frac{1}{1-R_{j}^{2}}$, $j=2,\dots,p$, where $R_{j}^{2}$ is the coefficient of determination of the auxiliary regression $\mathbf{X}_{j} =  \mathbf{X}_{-j} \cdot \boldsymbol{\alpha} + \mathbf{w}$ with $\mathbf{X}_{-j}$ being the matrix obtained when the independent variable  $\mathbf{X}_{j}$ is eliminated from matrix $\mathbf{X}$. It is considered that values of VIF higher than 10 indicate that the degree of multicollinearity is troubling (see, for example, @Marquardt1970 or @OBrien2007). @salmeron2018JSCS and @Salmeron2020a showed that the VIF is not an appropriate measure to detect linear relations between the intercept and other explanatory variables (non-essential collinearity).
* The Condition Number (CN) that is obtained with the following expression $CN(\mathbf{X}) = \sqrt{\frac{\mu_{max}}{\mu_{min}}}$ where $\mu_{max}$ and $\mu_{min}$ are the maximum and minimum eigenvalue of matrix $\mathbf{X}^{t} \mathbf{X}$. To obtain the eigenvalues, the matrix $\mathbf{X}$ has to be previously transformed in order to ensure that all its columns present unit length (see @salmeron2018JSCS for more details about this transformation). Values lower than 20 imply light collinearity, between 20 and 30 moderate collinearity, while values higher than 30 imply strong collinearity (see, for example, @Belsleyetal1980 and @Belsley1991).
Another alternative is to calculate the CN with and without the intercept with the goal of analyzing the contribution of the intercept.

Another set of measures to detect the existence of troubling multicollinearity are the matrix of simple linear correlations between the independent variables, $\mathbf{R} = \left( cor(X_{l}, X_{m}) \right)_{l,m=2,\dots,p}$ and its determinant, $| \mathbf{R} |$.
@garcia2019claudia show that values for the coefficient of simple correlation between the independent variables higher than $\sqrt{0.9}$ and determinant lower than $0.1013 + 0.00008626 \cdot n - 0.01384 \cdot p$ indicate a troubling degree of multicollinearity (see @Salmeron2020b or @Salmeron2022 for more details). 
The first value differs strongly from the threshold normally proposed equal to 0.7 to indicate a problem of near collinearity (see, for example, @HalkosTsilika). 

Also useful to use the coefficient of variation (CV), values less than 0.1002506 indicate the existence of troubling multicollinearity (see @Salmeron2020a for more details).

@garciaetal2016 and @salmeron2019velilla showed that the VIF is invariant to origin and scale changes, which is the same as saying that  model $\mathbf{Y} = \mathbf{X} \cdot \boldsymbol{\beta} + \mathbf{u}$ and the model $\mathbf{Y} = \mathbf{x} \cdot \boldsymbol{\beta} + \mathbf{u}$ present the same VIF, where $\mathbf{x} = [\mathbf{x}_{1} \ \mathbf{x}_{2} \dots \mathbf{x}_{p}]$ with $\mathbf{x}_{i} = \frac{\mathbf{X}_{i}-a_{i}}{b_{i}}$ for $a_{i} \in \mathbb{R}$, $b_{i}>0$ and $i=1,\dots,p$. Note that if $a_{i} = \overline{\mathbf{X}}_{i}$, $\mathbf{x}_{1}$ is a vector of zeros, i.e. the intercept disappears from the model. Instead, @salmeron2018 showed that the CN is not invariant to origin and scale changes, meaning that the two previous models present different CNs. This fact implies that models $\mathbf{Y} = \mathbf{X} \cdot \boldsymbol{\beta} + \mathbf{u}$ and $\mathbf{Y} = \mathbf{x} \cdot \boldsymbol{\beta} + \mathbf{u}$ present different eigenvalues.

Consequently, transforming the data in a linear regression model may affect the detection of the multicollinearity problem depending on the diagnostic measure. Furthermore, note that this sensitivity to scaling is due to the fact that there are certain transformations (such as data centering) that mitigate the multicollinearity problem, so that the dependence or otherwise on scaling simply highlights the capacity/incapacity of each measure to detect this reduction of the degree of near multicollinearity.

Therefore, when transforming the data in a linear regression model and analyzing whether the degree of multicollinearity is of concern or not, it is necessary to be clear whether the measure used to detect it is affected by the transformation and whether it is capable of detecting the two types of near multicollinearity mentioned (essential and non-essential). Thus, in this paper we analyze the MC index recently presented in @LinWangMueller2020.

First, this paper will briefly review the MC index. In order to show that the MC index depends on the transformation of the data and its inability to detect non-essential multicollinearity, we present two simulations with a troubling degree of essential and non-essential multicollinearity, respectively, and a third simulation where the degree of multicollinearity is not troubling.  For all these cases, we calculate the different measures to detect multicollinearity commented on the introduction together with the MC index. Two empirical applications recently applied in the scientific literature are also presented. After a discussion of the results we propose a scatter plot between the VIF and CV to detect which variables are the cause of the troubling degree of multicollinearity and the kind of multicollinearity (essential or non-essential) existing in the model. Finally, the main conclusions of the paper are summarized.

# Background: MC index

The MC index presented in @LinWangMueller2020 is based on the existing relation between the VIFs ant the inverse of the eigenvalues of the matrix $\mathbf{Z}^{t} \mathbf{Z}$ where $\mathbf{Z}$ represents the standardized matrix of $\mathbf{X}$. This is to say, is the matrix $\mathbf{x}$ mentioned in the introduction when for all $i$ it is obtained that $a_{i} = \overline{\mathbf{X}}_{i}$ and $b_{i} = \sqrt{n \cdot var \left( \mathbf{X}_{i} \right)}$ where $var \left( \mathbf{X}_{i} \right)$ represents the variance of $\mathbf{X}_{i}$.
    More precisely, taking into account the fact that in the main diagonal of $\left( \mathbf{Z}^{t} \mathbf{Z} \right)^{-1}$ we find the VIFs (for standardized data), it is possible to establish the following relation:
    
\begin{equation}
\left(
\begin{array}{c}
VIF(2) \\
\vdots \\
VIF(p)
\end{array}
\right) = \mathbf{A} \cdot \left(
\begin{array}{c}
\frac{1}{\mu_{2}} \\
\vdots \\
\frac{1}{\mu_{p}}
\end{array}
\right),
\end{equation}

where $\mathbf{A}$ is a matrix that depends on the eigenvalues of $\mathbf{Z}^{t} \mathbf{Z}$ and $\mu_{p}$ is the maximum eigenvalue of this matrix.

From this relationship, resampling and obtaining the regression of $1/\mu_{p}$ as a function of the VIFs, @LinWangMueller2020 proposed the use of the t-statistics to conclude which variable contributes the most to this relationship, thus identifying which variables are responsible for the degree of approximate multicollinearity in the model. These authors defined the MC index as *an index from zero to one, larger values indicating greater contribution of the variable i in explaining the observed severity of multicollinearity*.
   
Taking into account that the calculation of the MC index is based on the relation established between the VIFs and the inverse of the smallest eigenvalue, it seems logical to consider that the transformation of the data may affect the calculation of this measure. Thus, it is possible to conclude:

* If the MC index is calculated with a transformation of the data that leads to the elimination of  the intercept, the non-essential multicollinearity will be ignored.
* Due to the fact that MC index is based on the VIF, it should inherit its inability to detect the non-essential multicollinearity.

In conclusion, regardless of whether the data is transformed or not, the MC index is not capable of detecting non-essential multicollinearity. It is expected that it will show its usefulness in the case of essential multicollinearity.  
   
Therefore, when @LinWangMueller2020 commented that *there are different views on what centering technique is most appropriate in regression [...] To facilitate this diversity of opinion, in the software implementation of mcvis, we allow the option of passing matrices with different centering techniques as input. The role of scaling is not the focus of our work as our framework does not rely on any specific scaling method*, far from facilitating the use of their proposal, they consider scenarios for which the MC index is not designed, since in their theoretical development and step 2 of their method, the standardization of the data is performed.  However, as shown in this paper, the MC index is capable of detecting multicollinearity of the essential type only when used with its default option *studentise*.

# Simulations

In this section, different versions of the matrix $\mathbf{X} = [\mathbf{1} \ \mathbf{X}_{2} \ \mathbf{X}_{3} \ \mathbf{X}_{4}]$ will be simulated. The results on the correlation matrix, its determinant, condition number (with and without intercept), variance inflation factor and coefficient of variation are obtained using the \CRANpkg{multiColl} package (see @Salmeron2020b and @Salmeron2022 for more details).

In all cases, are calculated the values for the MC index for each set of simulated data considering the two alternative transformation for the data: *Euclidean* (centered by mean and divided by Euclidean length) and *studentise* (centered by mean and divided by standard deviation). In each case (simulation and kind of transformation), the calculation of the MC index was performed 100 times. 

## Simulation 1

In this case, 100 observations are generated according to $\mathbf{X}_{i} \sim N(10, 100)$, $i=2,3$, and $\mathbf{X}_{4} = \mathbf{X}_{3} - \mathbf{p}$ where $\mathbf{p} \sim N(1, 0.5)$. The goal of this simulation is to ensure that the variables  $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$ will be highly correlated (essential multicollinearity). This fact is confirmed when taking into account the following results in relation to the correlation matrix, correlation matrix's determinant, the CN with and without the intercept (with its corresponding increasing), the VIFs and the coefficient of variation of the different variables. 

```{r simulation 1}
  set.seed(2021)
  n = 100
  X2_S1 = rnorm(n, 10, 10)
  X3_S1 = rnorm(n, 10, 10)
  p = rnorm(n, 1, 0.5)
  X4_S1 = X3_S1 - p

  cte_S1 = rep(1, n)
  X_S1 = cbind(cte_S1, X2_S1, X3_S1, X4_S1)
```
```{r Results simulation 1, echo = TRUE}
  RdetR(X_S1)
  CNs(X_S1)
  VIF(X_S1)
  CVs(X_S1)
```

Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Simulation1randomhtml)', '\\@ref(tab:Simulation1randomlatex)'))` shows three random iterations, the average value of the 100 times and the standard deviation. As expected in the case of essential multicollinearity, from the average values of Simulation 1  it is noted that (specially with the transformation *studentise*) the MC index correctly identified that the variables $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$ are causing the troubling degree of essential multicollinearity. However, it is noted that in some cases the intercept or the variable $\mathbf{X}_{2}$ are identified as relevant in the existing linear relations when the *Euclidean* transformation is performed. This behavior is not observed when the *studentise* transformation is performed. This fact seems to indicate that the MC index depends on the transformation with the *studentise* transformation being the most appropriate.


```{r Simulation1MCIndex, include=FALSE}
  ite = 100
  esencial.mcvis.eu = matrix(-1, nrow = ite, ncol = 3) # almaceno en cada fila los MC index
  esencial.mcvis.st = matrix(-1, nrow = ite, ncol = 3)
  for (i in 1:ite){
    eu = mcvis(X_S1[,-1], standardise_method="euclidean")
    esencial.mcvis.eu[i,] = as.double(eu$MC[3,]) 
    #
    st = mcvis(X_S1[,-1], standardise_method="studentise")
    esencial.mcvis.st[i,] = as.double(st$MC[3,]) 
    #
  }
```

```{r Simulation1randomlatex, eval=knitr::is_latex_output(), tab.cap="MC index for Simulation 1."}
  Simulation1_random<-data.frame(
    "euclidean random 1" = c(round(c(esencial.mcvis.eu[26,]),digits=7)),
    "euclidean random 2" = c(round(c(esencial.mcvis.eu[23,]),digits=7)),
    "euclidean random 3" = c(round(c(esencial.mcvis.eu[57,]),digits=7)),
    "euclidean average" = c(round(as.numeric(colMeans(esencial.mcvis.eu),na.rm=T),digits=7)),
    "euclidean sd" = c(round(c(sd(esencial.mcvis.eu[,1]), sd(esencial.mcvis.eu[,2]), sd(esencial.mcvis.eu[,3])),digits=7)),
    "studentise random 1" = c(round(c(esencial.mcvis.st[37,]),digits=7)),
    "studentise random 2" = c(round(c(esencial.mcvis.st[6,]),digits=7)),
    "studentise random 3" = c(round(c(esencial.mcvis.st[3,]),digits=7)),
    "studentise average" = c(round(as.numeric(colMeans(esencial.mcvis.st),na.rm=T),digits=7)),
    "studentise sd" = c(round(c(sd(esencial.mcvis.st[,1]), sd(esencial.mcvis.st[,2]), sd(esencial.mcvis.st[,3])),digits=7))
  )
  Simulation1_random = t(Simulation1_random)
  rownames(Simulation1_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
  knitr::kable(Simulation1_random, format="latex", col.names=c("X2","X3","X4"), caption="MC index for the independent variables in Simulation 1. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the second and third variables is detected when studentise transformation is performed.", align ="cccc")
```

```{r Simulation1randomhtml, eval=knitr::is_html_output(), tab.cap="MC index for Simulation 1"}
  Simulation1_random<-data.frame(
    "euclidean random 1" = c(round(c(esencial.mcvis.eu[26,]),digits=7)),
    "euclidean random 2" = c(round(c(esencial.mcvis.eu[23,]),digits=7)),
    "euclidean random 3" = c(round(c(esencial.mcvis.eu[57,]),digits=7)),
    "euclidean average" = c(round(as.numeric(colMeans(esencial.mcvis.eu),na.rm=T),digits=7)),
    "euclidean sd" = c(round(c(sd(esencial.mcvis.eu[,1]), sd(esencial.mcvis.eu[,2]), sd(esencial.mcvis.eu[,3])),digits=7)),
    "studentise random 1" = c(round(c(esencial.mcvis.st[37,]),digits=7)),
    "studentise random 2" = c(round(c(esencial.mcvis.st[6,]),digits=7)),
    "studentise random 3" = c(round(c(esencial.mcvis.st[3,]),digits=7)),
    "studentise average" = c(round(as.numeric(colMeans(esencial.mcvis.st),na.rm=T),digits=7)),
    "studentise sd" = c(round(c(sd(esencial.mcvis.st[,1]), sd(esencial.mcvis.st[,2]), sd(esencial.mcvis.st[,3])),digits=7))
  )
  Simulation1_random = t(Simulation1_random)
  rownames(Simulation1_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
  knitr::kable(Simulation1_random, format="html", col.names=c("X2","X3","X4"), caption="MC index for the independent variables in Simulation 1. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the second and third variables is detected when studentise transformation is performed.", align ="cccc")
```
 
## Simulation 2

In this case, 100 observations are generated according to $\mathbf{X}_{i} \sim N(10, 100)$, $i=2,3$, and $\mathbf{X}_{4} \sim N(10, 0.0001)$. The goal of this simulation is to ensure that the variable $\mathbf{X}_{4}$ will be highly correlated to the intercept (non-essential multicollinearity). This fact is confirmed from the following results taking into account that @Salmeron2020a showed that a value of the CV lower than 0.1002506 indicates a troubling degree of non-essential multicollinearity. 

```{r simulation 2}
set.seed(2021)
n = 100
X2_S2 = rnorm(n, 10, 10)
X3_S2 = rnorm(n, 10, 10)
X4_S2 = rnorm(n, 10, 0.01)

cte_S2 = rep(1, n)
X_S2 = cbind(cte_S2, X2_S2, X3_S2, X4_S2)
```
```{r Results simulation 2,  echo = TRUE}
RdetR(X_S2)
CNs(X_S2)
VIF(X_S2)
CVs(X_S2)
```

Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Simulation2randomhtml)', '\\@ref(tab:Simulation2randomlatex)'))` presents three random iterations, the average value of the 100 times and the standard deviation for Simulation 2 for *Euclidean* and *studentise* transformations.  Before commenting the results of Simulation 2, it is important to take into account the fact that with transformations that imply the elimination of the intercept it will not be possible to detect the non-essential multicollinearity. 
Note that in some occasions, when *Euclidean* transformation is performed, it is concluded that $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$ are the most relevant while, when *studentise* transformation is performed, all variables seem to present the same relevance. In the first case, a higher stability is observed by considering the average values, although the conclusion is that there is a relation between  $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$ when the relationship is between the intercept and $\mathbf{X}_{4}$.

```{r Simulation 2 MC Index, include=FALSE}
ite = 100
nonesencial.mcvis.eu = matrix(-1, nrow = ite, ncol = 3) # almaceno en cada fila los MC index 
nonesencial.mcvis.st = matrix(-1, nrow = ite, ncol = 3)
for (i in 1:ite){
  eu = mcvis(X_S2[,-1], standardise_method="euclidean")
  nonesencial.mcvis.eu[i,] = as.double(eu$MC[3,]) 
  #
  st = mcvis(X_S2[,-1], standardise_method="studentise")
  nonesencial.mcvis.st[i,] = as.double(st$MC[3,]) 
  #
}
```


```{r Simulation2randomlatex, eval=knitr::is_latex_output(), tab.cap="MC index for Simulation 2"}
Simulation2_random<-data.frame(
  'euclidean random 1'=c(round(c(nonesencial.mcvis.eu[8,]),digits=7)),
  'euclidean random 2'=c(round(c(nonesencial.mcvis.eu[3,]),digits=7)),
  'euclidean random 3'=c(round(c(nonesencial.mcvis.eu[47,]),digits=7)),
  "euclidean average"=c(round(as.numeric(colMeans(nonesencial.mcvis.eu),na.rm=T),digits=7)),
  "euclidean sd" = c(round(c(sd(nonesencial.mcvis.eu[,1]), sd(nonesencial.mcvis.eu[,2]), sd(nonesencial.mcvis.eu[,3])),digits=7)),
  'studentise random 1'=c(round(c(nonesencial.mcvis.st[3,]),digits=7)),
  'studentise random 2'=c(round(c(nonesencial.mcvis.st[14,]),digits=7)),
  'studentise random 3'=c(round(c(nonesencial.mcvis.st[53,]),digits=7)),
  "studentise average"=c(round(as.numeric(colMeans(nonesencial.mcvis.st),na.rm=T),digits=7)),
  "studentise sd" = c(round(c(sd(nonesencial.mcvis.st[,1]), sd(nonesencial.mcvis.st[,2]), sd(nonesencial.mcvis.st[,3])),digits=7))
)
Simulation2_random = t(Simulation2_random)
rownames(Simulation2_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
knitr::kable(Simulation2_random, format="latex", col.names =c("X2","X3","X4"),  caption="MC index for the independent variables in Simulation 2. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the four variable and the intercept is not detected.", align ="ccccc")
```

```{r Simulation2randomhtml, eval=knitr::is_html_output(), tab.cap="MC index for Simulation 2"}
Simulation2_random<-data.frame(
  'euclidean random 1'=c(round(c(nonesencial.mcvis.eu[8,]),digits=7)),
  'euclidean random 2'=c(round(c(nonesencial.mcvis.eu[3,]),digits=7)),
  'euclidean random 3'=c(round(c(nonesencial.mcvis.eu[47,]),digits=7)),
  "euclidean average"=c(round(as.numeric(colMeans(nonesencial.mcvis.eu),na.rm=T),digits=7)),
  "euclidean sd" = c(round(c(sd(nonesencial.mcvis.eu[,1]), sd(nonesencial.mcvis.eu[,2]), sd(nonesencial.mcvis.eu[,3])),digits=7)),
  'studentise random 1'=c(round(c(nonesencial.mcvis.st[3,]),digits=7)),
  'studentise random 2'=c(round(c(nonesencial.mcvis.st[14,]),digits=7)),
  'studentise random 3'=c(round(c(nonesencial.mcvis.st[53,]),digits=7)),
  "studentise average"=c(round(as.numeric(colMeans(nonesencial.mcvis.st),na.rm=T),digits=7)),
  "studentise sd" = c(round(c(sd(nonesencial.mcvis.st[,1]), sd(nonesencial.mcvis.st[,2]), sd(nonesencial.mcvis.st[,3])),digits=7))
)
Simulation2_random = t(Simulation2_random)
rownames(Simulation2_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
knitr::kable(Simulation2_random, format="html", col.names =c("X2","X3","X4"),  caption="MC index for the independent variables in Simulation 2. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the four variable and the intercept is not detected.", align ="ccccc")
```
 
## Simulation 3

Finally, in this case 100 observations are generated according to $\mathbf{X}_{i} \sim N(10, 100)$, $i=2,3,4$. The goal of this simulation is to ensure that the degree of multicollinearity (essential and non-essential) will be not troubling. This fact is confirmed when taking into account the following results.  

```{r simulation 3}
set.seed(2021)
n = 100
X2_S3 = rnorm(n, 10, 10)
X3_S3 = rnorm(n, 10, 10)
X4_S3 = rnorm(n, 10, 10)

cte_S3 = rep(1, n)
X_S3 = cbind(cte_S3, X2_S3, X3_S3, X4_S3)    
```
```{r Results simulation 3, echo = TRUE}
RdetR(X_S3)
CNs(X_S3)
VIF(X_S3)
CVs(X_S3)
```

Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Simulation3randomhtml)', '\\@ref(tab:Simulation3randomlatex)'))` presents three random iterations, the average value of the 100 times and the standard deviation for Simulation 3 for *Euclidean* and *studentise* transformations. Simulation 3 shows different situations depending on the transformation: when the *Euclidean* transformation is performed, the variable $\mathbf{X}_{3}$ is also identified apart from variable $\mathbf{X}_{4}$; with the *studentise* transformation, all the variables seem to be relevant.

```{r Simulation 3 MC Index, include=FALSE}
ite = 100
without.mcvis.eu = matrix(-1, nrow = ite, ncol = 3) # almaceno en cada fila los MC index 
without.mcvis.st = matrix(-1, nrow = ite, ncol = 3)
for (i in 1:ite){
  eu = mcvis(X_S3[,-1], standardise_method="euclidean")
  without.mcvis.eu[i,] = as.double(eu$MC[3,]) 
  #
  st = mcvis(X_S3[,-1], standardise_method="studentise")
  without.mcvis.st[i,] = as.double(st$MC[3,]) 
  #
}
```

```{r Simulation3randomhtml, eval=knitr::is_html_output(), tab.cap="MC index for Simulation 3"}
Simulation3_random<-data.frame(
  'euclidean random 1'=c(round(c(without.mcvis.eu[2,]),digits=7)),
  'euclidean random 2'=c(round(c(without.mcvis.eu[43,]),digits=7)),
  'euclidean random 3'=c(round(c(without.mcvis.eu[51,]),digits=7)),
  "euclidean average"=c(round(as.numeric(colMeans(without.mcvis.eu),na.rm=T),digits=7)),
  "euclidean sd" = c(round(c(sd(without.mcvis.eu[,1]), sd(without.mcvis.eu[,2]), sd(without.mcvis.eu[,3])),digits=7)),
  'studentise random 1'=c(round(c(without.mcvis.st[1,]),digits=7)),
  'studentise random 2'=c(round(c(without.mcvis.st[46,]),digits=7)),
  'studentise random 3'=c(round(c(without.mcvis.st[51,]),digits=7)),
  "studentise average"=c(round(as.numeric(colMeans(without.mcvis.st),na.rm=T),digits=7)),
  "studentise sd" = c(round(c(sd(without.mcvis.st[,1]), sd(without.mcvis.st[,2]), sd(without.mcvis.st[,3])),digits=7))
)
Simulation3_random = t(Simulation3_random)
rownames(Simulation3_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
knitr::kable(Simulation3_random, format="html", col.names =c("X2","X3","X4"),  caption="MC index for the independent variables in Simulation 3. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The not troubling relationship between the variables is not detected.", align ="cccc")
```

```{r Simulation3randomlatex, eval=knitr::is_latex_output(), tab.cap="MC index for Simulation 3"}
Simulation3_random<-data.frame(
  'euclidean random 1'=c(round(c(without.mcvis.eu[2,]),digits=7)),
  'euclidean random 2'=c(round(c(without.mcvis.eu[43,]),digits=7)),
  'euclidean random 3'=c(round(c(without.mcvis.eu[51,]),digits=7)),
  "euclidean average"=c(round(as.numeric(colMeans(without.mcvis.eu),na.rm=T),digits=7)),
  "euclidean sd" = c(round(c(sd(without.mcvis.eu[,1]), sd(without.mcvis.eu[,2]), sd(without.mcvis.eu[,3])),digits=7)),
  'studentise random 1'=c(round(c(without.mcvis.st[1,]),digits=7)),
  'studentise random 2'=c(round(c(without.mcvis.st[46,]),digits=7)),
  'studentise random 3'=c(round(c(without.mcvis.st[51,]),digits=7)),
  "studentise average"=c(round(as.numeric(colMeans(without.mcvis.st),na.rm=T),digits=7)),
  "studentise sd" = c(round(c(sd(without.mcvis.st[,1]), sd(without.mcvis.st[,2]), sd(without.mcvis.st[,3])),digits=7))
)
Simulation3_random = t(Simulation3_random)
rownames(Simulation3_random)=c("Euclidean - Random 1", "Euclidean - Random 2", "Euclidean - Random 3", "Euclidean - Average", "Euclidean - Standard Deviation", "Studentise - Random 1", "Studentise - Random 2", "Studentise - Random 3", "Studentise - Average", "Studentise - Standard Deviation")
knitr::kable(Simulation3_random, format="latex", col.names =c("X2","X3","X4"),  caption="MC index for the independent variables in Simulation 3. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The not troubling relationship between the variables is not detected.", align ="cccc")
```
 
## Interpretation of the obtained results

From the above results, it is concluded that the MC index applied individually is not able to detect if the degree of multicollinearity is troubling. This conclusion is in line with the comment presented by @LinWangMueller2020 where it is stated that *those classical collinearity measures are used together with mcvis for the better learning of how one or more variables display dominant behavior in explaining multicollinearity*. That is to say, it is recommended to use measures such as the VIF and the CN to detect whether the degree of multicollinearity is troubling and, if it is, then use the MC index to detect which variables are more relevant. 

We should reiterate the fact that the results of the MC index depend on the transformation performed with the data. 

Finally, it is worthy of note that the lowest dispersion is obtained when the *studentise* transformation is performed, which indicate that with this transformation a higher stability exists in the results obtained with the 100 iterations performed.  

# Examples

In this section we will analyze two examples applied recently to illustrate the multicollinearity problem. The first one focuses on the existence of non-essential approximate multicollinearity while the second one is focused on essential multicollinearity.

## Example 1

@Salmeron2020a analyzed the Euribor as a function of the harmonized index of consumer prices (**HICP**), the balance of payments to net current account (**BC**) and the government deficit to net non-financial accounts (**GD**).

```{r Example1Datalatex, eval = knitr::is_latex_output(), tab.cap="Data set for example 1", include=FALSE}
table01=data.frame(
  Euribor=c(3.63,3.90,3.45,3.01,2.54,2.23,2.20,2.36,2.14,2.29,2.35,2.32,2.32,2.19,2.20,2.63,2.95,3.31,3.62,3.60,4.09,4.38,4.65,4.68,4.48,5.05,5.37,4.34,2.22,1.67,1.34,1.24,1.22,1.25,1.40,1.52,1.74,2.13,2.11,2.06,1.67,1.28,0.90,0.60,0.57,0.51,0.54),
  HIPC=c(92.92,  93.85,  93.93,  94.41,  95.08,  95.73,  95.90,  96.40,  96.77,  97.97,  98.06,  98.67,  
         98.76,  99.96, 100.30, 100.97, 101.07, 102.44, 102.52, 102.79, 102.97, 104.38, 104.45, 105.77, 
         106.43, 108.18, 108.49, 108.21, 107.46, 108.37, 108.08, 108.67, 108.67, 110.12, 109.95, 110.87, 
         111.36, 113.15, 112.91, 114.12, 114.35, 115.93, 115.78, 116.75, 116.47, 117.55, 117.34),
  BC=c(17211,   2724,  17232,   9577,   4117,  -2134,   6117,  10949,  18360,  13646,   8424,  14319,   
       3885,   4493,   -320,  -2736,  -6909,  -4848,  -4255,   1347,   8781,   8723,   3662, -17548, 
       -37041, -27624, -37723, -43584, -16070, -5029,   7294,     85,  -4399,  -2431,   2137,  -4345, 
       -12643,  -2272,  -3592,   8071,  12202,  35619,  42161,  43880,  52483,  56376,  48981),
  GD=c(-51384.0,  -49567.1,  -52128.4,  -53593.3,  -65480.0,  -50343.8,  -75646.4,  -59120.8,  -69246.3,  
       -60313.8,  -56782.9,  -55313.1,  -67034.4,  -61942.8,  -46258.4,  -43761.4,  -37562.6,  -35609.6,  
       -27064.0,  -32497.2,  -18389.0,   -9923.5,   -9727.0,  -23729.9,  -28909.3,  -46527.0,  -49654.0,  
       -81729.7, -121227.5, -142580.9, -164699.2, -152269.2, -162477.4, -128366.4, -169848.0, -129290.2, 
       -104646.7, -103143.8, -102621.8, -104240.4, -82309.3,  -91620.9,  -85054.4,  -99998.2,  -81287.1,  -77738.8,  -73003.3)
)
#knitr::kable(table01, format="latex", caption="Data set for example 1. @Salmeron2020a", align ="cccc")
```

```{r Example1Datahtml, eval=knitr::is_html_output(), tab.cap="Data set for example 1", include=FALSE}
table01=data.frame(
  Euribor=c(3.63,3.90,3.45,3.01,2.54,2.23,2.20,2.36,2.14,2.29,2.35,2.32,2.32,2.19,2.20,2.63,2.95,3.31,3.62,3.60,4.09,4.38,4.65,4.68,4.48,5.05,5.37,4.34,2.22,1.67,1.34,1.24,1.22,1.25,1.40,1.52,1.74,2.13,2.11,2.06,1.67,1.28,0.90,0.60,0.57,0.51,0.54),
  HIPC=c(92.92,  93.85,  93.93,  94.41,  95.08,  95.73,  95.90,  96.40,  96.77,  97.97,  98.06,  98.67,  
         98.76,  99.96, 100.30, 100.97, 101.07, 102.44, 102.52, 102.79, 102.97, 104.38, 104.45, 105.77, 
         106.43, 108.18, 108.49, 108.21, 107.46, 108.37, 108.08, 108.67, 108.67, 110.12, 109.95, 110.87, 
         111.36, 113.15, 112.91, 114.12, 114.35, 115.93, 115.78, 116.75, 116.47, 117.55, 117.34),
  BC=c(17211,   2724,  17232,   9577,   4117,  -2134,   6117,  10949,  18360,  13646,   8424,  14319,   
       3885,   4493,   -320,  -2736,  -6909,  -4848,  -4255,   1347,   8781,   8723,   3662, -17548, 
       -37041, -27624, -37723, -43584, -16070, -5029,   7294,     85,  -4399,  -2431,   2137,  -4345, 
       -12643,  -2272,  -3592,   8071,  12202,  35619,  42161,  43880,  52483,  56376,  48981),
  GD=c(-51384.0,  -49567.1,  -52128.4,  -53593.3,  -65480.0,  -50343.8,  -75646.4,  -59120.8,  -69246.3,  
       -60313.8,  -56782.9,  -55313.1,  -67034.4,  -61942.8,  -46258.4,  -43761.4,  -37562.6,  -35609.6,  
       -27064.0,  -32497.2,  -18389.0,   -9923.5,   -9727.0,  -23729.9,  -28909.3,  -46527.0,  -49654.0,  
       -81729.7, -121227.5, -142580.9, -164699.2, -152269.2, -162477.4, -128366.4, -169848.0, -129290.2, 
       -104646.7, -103143.8, -102621.8, -104240.4, -82309.3,  -91620.9,  -85054.4,  -99998.2,  -81287.1,  -77738.8,  -73003.3)
)
#knitr::kable(table01, format="html", caption="Data set for example 1. @Salmeron2020a", align ="cccc")
```

The following determinant of the matrix of correlations of the independent variables, the VIFs, condition number without intercept and with intercept  and the coefficients of variation indicate that the degree of essential near multicollinearity is not troubling while the non-essential type (due to the variable **HIPC**) is. 

```{r, include=FALSE}
HIPC = c(92.92,  93.85,  93.93,  94.41,  95.08,  95.73,  95.90,  96.40,  96.77,  97.97,  98.06,  98.67,  
         98.76,  99.96, 100.30, 100.97, 101.07, 102.44, 102.52, 102.79, 102.97, 104.38, 104.45, 105.77, 
         106.43, 108.18, 108.49, 108.21, 107.46, 108.37, 108.08, 108.67, 108.67, 110.12, 109.95, 110.87, 
         111.36, 113.15, 112.91, 114.12, 114.35, 115.93, 115.78, 116.75, 116.47, 117.55, 117.34)
BC = c(17211,   2724,  17232,   9577,   4117,  -2134,   6117,  10949,  18360,  13646,   8424,  14319,   
       3885,   4493,   -320,  -2736,  -6909,  -4848,  -4255,   1347,   8781,   8723,   3662, -17548, 
       -37041, -27624, -37723, -43584, -16070, -5029,   7294,     85,  -4399,  -2431,   2137,  -4345, 
       -12643,  -2272,  -3592,   8071,  12202,  35619,  42161,  43880,  52483,  56376,  48981)
GD = c(-51384.0,  -49567.1,  -52128.4,  -53593.3,  -65480.0,  -50343.8,  -75646.4,  -59120.8,  -69246.3,  
       -60313.8,  -56782.9,  -55313.1,  -67034.4,  -61942.8,  -46258.4,  -43761.4,  -37562.6,  -35609.6,  
       -27064.0,  -32497.2,  -18389.0,   -9923.5,   -9727.0,  -23729.9,  -28909.3,  -46527.0,  -49654.0,  
       -81729.7, -121227.5, -142580.9, -164699.2, -152269.2, -162477.4, -128366.4, -169848.0, -129290.2, 
       -104646.7, -103143.8, -102621.8, -104240.4, -82309.3,  -91620.9,  -85054.4,  -99998.2,  -81287.1,  -77738.8,  -73003.3)
```

```{r Example1, include=FALSE}
n = length(HIPC)
cte = rep(1, n)
data1 = cbind(cte, HIPC, BC, GD)
```

```{r Results Example1, include=FALSE, echo = TRUE}
RdetR(data1)
CNs(data1)
VIF(data1)
CVs(data1)
```

Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:Example1-interactive)', '\\@ref(fig:Example1-static)'))` shows a tour displayed with a scatterplot by using the \CRANpkg{tourr} package. This package allows  tours of multivariate data, see @tourr for more details. From the tour on all the explanatory variables (it runs for 3.47 minutes in the html version),  no linear relation is observed between the explanatory variables. Note that this package does not allow us to work with the intercept.

```{r, include=FALSE}
render(data1[,-1], grand_tour(d=2), display_xy(), "png",
       apf=2, frames=1, "Example1plot1.png"
)
render(data1[,-1], grand_tour(d=2), display_xy(), "png",
       apf=30, frames=10, "Example1plot2.png"
)
render(data1[,-1], grand_tour(d=2), display_xy(), "png",
       apf=40, frames=10, "Example1plot3.png"
)
render(data1[,-1], grand_tour(d=2), display_xy(), "png",
       apf=50, frames=10, "Example1plot4.png"
)
```

```{r Example1-static, eval=knitr::is_latex_output(), out.width="50%", fig.show="hold", fig.cap="Representation  of example 1 using the R tourr package. No linear relationship is observed between the explanatory variables in any of the four plots. See html version for an interactive 2-D tour."}
knitr::include_graphics(rep(c("Example1plot1.png","Example1plot2.png","Example1plot3.png","Example1plot4.png"),1))
```


```{r, include=FALSE}
render_gif(data1[,-1], grand_tour(d=2), display_xy(), gif_file="animation1.gif", apf=1/10, frames=2085, start=NULL)
```

```{r Example1-interactive, eval=knitr::is_html_output(), fig.cap="An interactive 2-D tour for example 1 by using the R tourr package.  From the 3.47 minutes tour on all the explanatory variables no linear relation is observed between the explanatory variables. Note that the intercept is not considered"}
knitr::include_graphics("animation1.gif")
``` 
 
In relation to the application of the \CRANpkg{mcvis} package in this example, it can be observed that if the *Euclidean* transformation is performed the method concludes that all the variables seem to be relevant: 

```{r Example1_MCindex euclidian, tab.cap="MC index results for example 1 with Euclidean transformation"}
set.seed(2022)
mcvis(data1[,-1], standardise_method="euclidean")
```

For *studentise* transformation it concludes the establishing of a relationship between **HIPC** and **GD**:

```{r Example1_MCindex studentise, tab.cap="MC index results for example 1 with Studentise transformation"}
mcvis(data1[,-1], standardise_method="studentise")
```

Clearly, these conclusions are not consistent. Moreover, by eliminating the intercept when transforming the data, it is not feasible to detect the non-essential multicollinearity.

## Example 2 
 
@HastieTibshirani illustrated multicollinearity with a data set which contains information about the dependent variable **balance** (average credit card debt for a number of individuals) and, among others, the following quantitative independent variables: **income**, **limit** (credit limit), **rating** (credit rating), **cards** (number of credit cards), **age**, **education** (years of education).  Data is available in the \CRANpkg{ISLR} package (see @ISLR for more details). 

The following determinant of the matrix of correlations of the independent variables, the simple correlation between **limit** and **rating** (0.99687974), the VIFs, condition number without intercept and with intercept and CVs indicate that the degree of approximate multicollinearity of the non-essential type is not troubling while that of the essential type  (due to the relationship between variables **limit** and **rating**) is troubling. 

```{r Example2, include=FALSE}
attach(Credit)
#str(Credit)

n = length(Balance)
cte = rep(1, n)
data2 = cbind(cte, Income, Limit, Rating, Cards, Age, Education)
```

```{r Results Example2, include=FALSE, echo = TRUE}  
RdetR(data2)
CNs(data2)
VIF(data2)
CVs(data2)
```

Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:Example2-interactive)', '\\@ref(fig:Example2-static)'))` displays its  tour by using again \CRANpkg{tourr} package. Multicollinearity was checked using a tour on all the explanatory variables (it runs for 3.47 minutes in html version). In this case a certain linear relationship is observed, although it is difficult to determine which variables are related.

```{r, include=FALSE}
render(data2[,-1], grand_tour(d=2), display_xy(), "png",
       apf=2, frames=1, "Example2plot1.png"
)
render(data2[,-1], grand_tour(d=2), display_xy(), "png",
       apf=30, frames=10, "Example2plot2.png"
)
render(data2[,-1], grand_tour(d=2), display_xy(), "png",
       apf=40, frames=10, "Example2plot3.png"
)
render(data2[,-1], grand_tour(d=2), display_xy(), "png",
       apf=50, frames=10, "Example2plot4.png"
)
```

```{r Example2-static, eval=knitr::is_latex_output(), out.width="50%", fig.show="hold", fig.cap="Representation  of example 2 using the R tourr package. From the 3.47 minutes tour on all the explanatory variables, a certain linear relationship between independent variables is observed specially in plots one (top and left) and four (bottom and right). See html version for an interactive 2-D tour."}
knitr::include_graphics(rep(c("Example2plot1.png","Example2plot2.png","Example2plot3.png","Example2plot4.png"),1))
```

```{r, include=FALSE}
render_gif(data2[,-1], grand_tour(d=2), display_xy(), gif_file="animation2.gif", apf=1/10, frames=2085, start=NULL)
```

```{r Example2-interactive, eval=knitr::is_html_output(), fig.cap="An interactive 2-D tour for example 2 by using the R tourr package. From the 3.47 minutes tour on all the explanatory variables a certain linear relationship bewteen independent variables is observed between the explanatory variables. Note that the intercept is not considered"}
knitr::include_graphics("animation2.gif")
```

In relation to the application of the \CRANpkg{mcvis} package in this example, it can be observed that if the *Euclidean* transformation is performed the variable with the highest value is **income**, with the variable **rating** being the second-lowest in value. 

```{r Example2_MCindex euclidian, tab.cap="MC index results for example 2 with Euclidean transformation"}
set.seed(2022)
mcvis(data2[,-1], standardise_method="euclidean")
```

Finally, when the *studentise* transformation is applied, the method clearly indicates that variables **limit** and **rating** are the ones responsible for the multicollinearity problem.

```{r Example2_MCindex studentise, tab.cap="MC index results for example 2 with Studentise transformation"}
mcvis(data2[,-1], standardise_method="studentise")
```
 
# Discussion  

The results shown in the previous sections indicate that, on the one hand, the calculation of the MC index depends on the transformation performed and, on the other hand, that to apply this index with guarantees, the *studentise* transformation is the most appropriate.

It was also showed that the MC index ``inherits'' the same limitations indicated by @LinWangMueller2020 when they state that *collinearity indices such as the variance inflation factor and the condition number have limitations and may not be effective in some applications*. In the case of VIF, these limitations are well summarized by @LinWangMueller2020: *The VIF can show how variables are correlated to each other, but as shown, low correlation does not guarantee low level of collinearity*.
Note that the example applied by the authors in Section 1 (similar to the one presented in @salmeron2018JSCS and a reduced version of the one presented by @Belsley1984) is a clear case in which the multicollinearity is non-essential. As was commented earlier, the VIF ([@salmeron2018JSCS] and [@Salmeron2020a]) and the MC index are not able to detect this kind of multicollinearity, for this reason they are only recommended for detecting essential multicollinearity.

Another limitation of the VIF (and also of the MC index) worthy of note is that it is not adequate for calculating with dummy variables. Using these kinds of variables, the VIF is obtained from a coefficient of determination of an auxiliary regression whose dependent variable is a dummy variable. Although it is possible to apply ordinary least squares in this kind of regression, it is well known that it can present some problems: for example, the coefficient of determination is not representative since it measures the linear relation but the relation between the dummy variable and the rest of independent variables is not linear. For this reason, these kinds of regressions are estimated with non-linear models such as the logit/probit. Thus, we consider that if it is not adequate for calculating the VIF associated to a dummy variable,  these kinds of variables should be avoided in the calculation of the MC index.

Finally, Simulation 3  shows that MC index is not able to detect whether the degree of essential multicollinearity is troubling. For this reason, it should only be used once this situation is determined by other measures (as set out in the introduction) and in order to determine which variables cause it.

# Solution

The distinction between essential and non-essential multicollinearity and the limitations of each measure for detecting the different kinds of multicollinearity, can be very useful for detecting whether there is a troubling degree of multicollinearity, what kind of multicollinearity it is and which variables are causing the multicollinearity. Thus, taking into account the fact that the VIF is useful for detecting essential multicollinearity and the CV is useful for detecting non-essential multicollinearity, the scatter plot of both measures can provide interesting information in a joint way.

We present the scatter plot for the values of the VIF and the CV  for the three simulations previously performed. Note that the figures include the lines corresponding to the established thresholds for each measure: red dashed vertical line for 0.1002506 (CV) and red dotted horizontal line for 10 (VIF). These lines determine four regions that can be interpreted as follows: A, existence of troubling non-essential and non-troubling essential multicollinearity; B, existence of troubling essential and non-essential multicollinearity; C, existence of non-troubling non-essential and troubling essential multicollinearity; D: non-troubling degree of existing multicollinearity (essential and non-essential).

Considering this classification, in Simulation 1 (Figure \@ref(fig:FigureS1)) it is noted that there is a troubling degree of essential multicollinearity due to the variables $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$, while in Simulation 2 (Figure \@ref(fig:FigureS2)) it is noted that there is a troubling degree of non-essential multicollinearity due to the variable $\mathbf{X}_{4}$ and in Simulation 3 (Figure \@ref(fig:FigureS3)) it is noted that the degree of multicollinearity is not troubling.

```{r FigureS1,  fig.cap="Representation of the VIFs and CVs of explanatory variables in Simulation 1.  A troubling degree of essential multicollinearity due to variables third and four is detected by considering thresholds of 0.1002506 (CV) and 10  (VIF) highlighted with red lines. Note that VIFs of variables three and four are greater than 10 and the CV of variable 2 is lower than 0.1002506."}
vifs = VIF(X_S1)
cvs = CVs(X_S1)
plot(cvs, vifs, col="blue", lwd=3, xlim=c(0, max(cvs)), ylim=c(0, max(vifs)), xlab="Coefficient of Variation", ylab="Variance Inflation Factor")
abline(h=10, col="red", lwd=2, lty=2)
# lty: 0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash
abline(v=0.1002506, col="red", lwd=2, lty=3)
text(0.05, 3, "A")
text(0.05, (10+max(vifs))/2, "B")
text((0.01+max(cvs))/2, (10+max(vifs))/2, "C")
text((0.01+max(cvs))/2, 3, "D")
text(cvs, vifs+12, labels = c("2","3","4"))
```

```{r FigureS2, fig.cap="Representation of the VIFs and CVs of explanatory variables in Simulation 2.  A troubling degree of non-essential multicollinearity due to variable four is detected by considering thresholds of 0.1002506 (CV) and 10  (VIF) highlighted with red lines. Note that the VIFs of variables three and four are less than 10 and the CV greater than 0.1002506; while the CV of second variable is less than 0.1002506."}
vifs = VIF(X_S2)
cvs = CVs(X_S2)
plot(cvs, vifs, col="blue", lwd=3, xlim=c(0, max(cvs)), ylim=c(0, max(vifs)+10), xlab="Coefficient of Variation", ylab="Variance Inflation Factor")
abline(h=10, col="red", lwd=2, lty=2)
abline(v=0.1002506, col="red", lwd=2, lty=3)
text(0.05, 3, "A")
text(0.05, (10+max(vifs)+10)/2, "B")
text((0.01+max(cvs))/2, (10+max(vifs)+10)/2, "C")
text((0.01+max(cvs))/2, 3, "D")
text(cvs+0.03, vifs, labels = c("2","3","4"))
```

```{r FigureS3, fig.cap="Representation of the VIFs and CVs of explanatory variables in Simulation 3. The degree of  multicollinearity is not troubling by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of all variables are lower than 10 and the CVs greater than 0.1002506."}
vifs = VIF(X_S3)
cvs = CVs(X_S3)
plot(cvs, vifs, col="blue", lwd=3, xlim=c(0, max(cvs)), ylim=c(0, max(vifs)+10), xlab="Coefficient of Variation", ylab="Variance Inflation Factor")
abline(h=10, col="red", lwd=2, lty=2)
abline(v=0.1002506, col="red", lwd=2, lty=3)
text(0.05, 3, "A")
text(0.05, (10+max(vifs)+10)/2, "B")
text((0.01+max(cvs))/2, (10+max(vifs)+10)/2, "C")
text((0.01+max(cvs))/2, 3, "D")
text(cvs, vifs+0.4, labels = c("2","3","4"))
```

Analogously, we present Figures \@ref(fig:FigureExample1) and \@ref(fig:FigureExample2) for Examples 1 and 2, respectively. For Example 1, it is concluded that the only type of troubling multicollinearity  is non-essential due to the second variable (**HIPC**). On the other hand, for Example 2 it is concluded that the only type of troubling multicollinearity  is that  essentially due to the relationship existing between the third and fourth variables (**limit** and **rating**).

```{r FigureExample1, fig.cap="Representation of the VIFs and CVs of the variables of example 1. A troubling degree of non-essential multicollinearity generated by variable 2 (HIPC) is detected by considering thresholds of 0.1002506 (CV) and 10  (VIF) highlighted with red lines. Note that the VIFs of variables three and four are less than 10 and the CV greater than 0.1002506; while the CV of second variable is less than 0.1002506."}
vifs = VIF(data1)
cvs = CVs(data1)
plot(cvs, vifs, col="blue", lwd=3, xlim=c(0, max(cvs)), ylim=c(0, max(vifs)+10), xlab="Coefficient of Variation", ylab="Variance Inflation Factor")
abline(h=10, col="red", lwd=2, lty=2)
abline(v=0.1002506, col="red", lwd=2, lty=3)
text(0.05, 3, "A")
text(0.05, (10+max(vifs)+10)/2, "B")
text((0.01+max(cvs))/2, (10+max(vifs)+10)/2, "C")
text((0.01+max(cvs))/2, 3, "D")
text(cvs, vifs+0.4, labels = c("2","3","4"))
```

```{r FigureExample2, fig.cap="Representation of the VIFs and CVs of the variables of example 2. A troubling degree of essential multicollinearity generated by variables three and four (limit and rating) is detected by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of variables three and four are greater than 10 while the VIFs and CVs of the rest of variables are, respectively, lower than 10 and greater than 0.1002506."}
vifs = VIF(data2)
cvs = CVs(data2)
plot(cvs, vifs, col="blue", lwd=3, xlim=c(0, max(cvs)), ylim=c(0, max(vifs)+10), xlab="Coefficient of Variation", ylab="Variance Inflation Factor")
abline(h=10, col="red", lwd=2, lty=2)
abline(v=0.1002506, col="red", lwd=2, lty=3)
text(0.05, 3, "A")
text(0.05, (10+max(vifs))/2, "B")
text((0.01+max(cvs))/2, (10+max(vifs)+10)/2, "C")
text((0.01+max(cvs))/2, 3, "D")
text(cvs-0.02, vifs+0.4, labels = c("2","3","4","5","6","7"))
```

# Summary

This paper analyses the limitations that may arise in detecting the problem of troubling multicollinearity in a linear regression model due to transformations performed on the data. The discussion is used to illustrate that the MC index presented by @LinWangMueller2020 depends on the way that the data are transformed.

It was shown that when the *studentise* transformation is performed, the measure is stable for *measuring what variable contributes the most to the linear relationship, and thus identifying what variables best explain the collinearity in the original data* [@LinWangMueller2020], as long as the multicollinearity is essential. This kind of transformation was taken as default by the authors and this paper contributes with a formal justification. Note that the MC index only provides interesting information if the troubling multicollinearity is previously detected using other measures (such as the VIF or the CN).

Summarizing, the achievement of the goal intended by the \CRANpkg{mcvis} package is conditioned by the following limitations:
  
  * It is not able to detect non-essential multicollinearity.
* Other measures (as the VIF or CN) should be  applied previously to determine whether the degree of essential multicollinearity is troubling.
* The *studentise* transformation should be applied.
* It is not applicable for dummy variables.

Finally, it is proposed to use a scatter plot between the VIFs and the CVs, on the one hand, to detect whether the degree of multicollinearity (essential or not essential) is troubling and, on the other hand, to detect which variables are causing the multicollinearity. This would overcome the limitations of the \CRANpkg{mcvis} package discussed above while achieving its overall purpose.

# Acknowledgments

This work has been supported by project PP2019-EI-02 of the University of Granada (Spain), by project A-SEJ-496-UGR20 of the Andalusian Government's Counseling of Economic Transformation, Industry, Knowledge and Universities (Spain) and by project I+D+i PID2019-107767GA-I0 financed by MCIN/AEI/10.13039/501100011033.



